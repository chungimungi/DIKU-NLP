{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c40900",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2725f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open(\"../week-41/test_question.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "df_test = pd.DataFrame(test_data[\"questions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97350d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sarene/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sarene/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/sarene/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/sarene/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# other imports\n",
    "import nltk\n",
    "nltk.download('punkt_tab')      \n",
    "nltk.download('wordnet')    \n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForTokenClassification, AutoModelForSequenceClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments, EarlyStoppingCallback, DataCollatorForSeq2Seq\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter, defaultdict\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os, gc\n",
    "import evaluate\n",
    "\n",
    "from evaluate import load\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7c1393",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "lang_codes = {\n",
    "    \"pt\": \"por_Latn\",\n",
    "    \"hi\": \"hin_Deva\",\n",
    "    \"ja\": \"jpn_Jpan\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f7a0fe",
   "metadata": {},
   "source": [
    "#### System instructions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7cf21",
   "metadata": {},
   "source": [
    "Objective: You are tasked with creating a\n",
    "dataset containing 10 items. Each item must ad-\n",
    "here to the following specifications:\n",
    "\n",
    "• Question: the question text written in the\n",
    "specified language.\n",
    "\n",
    "• Answer: provided in both the language of\n",
    "the question and in English.\n",
    "\n",
    "• Context Paragraph (English): a short\n",
    "English paragraph that provides background\n",
    "information relevant to the question.\n",
    "Content Requirements:\n",
    "\n",
    "• The dataset must include a mix of easy, hard,\n",
    "and unanswerable questions.\n",
    "\n",
    "• Do not repeat context paragraphs.\n",
    "\n",
    ">Formatting Requirements: <br>\n",
    ">{                                                   <br>\n",
    ">  \"question\": \"\\<question text\\>\",<br>\n",
    ">  \"context\": \"\\<english context paragraph\\>\",<br>\n",
    ">  \"lang\": \"\\<language of question\\>\",<br>\n",
    ">  \"answerable\": \"\\<true if the question is answerable given the context\\>\" <br>\n",
    ">  \"answer_start\": \"\",<br>\n",
    ">  \"answer\": \"\\<if answerable, answer in english\\>\",<br>\n",
    ">  \"answer_inlang\": \"\\<answer translated into the question language\\>\" <br>\n",
    ">}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b152a",
   "metadata": {},
   "source": [
    "## Part 1: Rule-based Classifier \n",
    "Week 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccbc0209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating pt: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]\n",
      "Translating pt: 100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n",
      "Predicting pt: 100%|██████████| 10/10 [00:01<00:00,  8.54it/s]\n",
      "Translating hi: 100%|██████████| 2/2 [00:00<00:00,  3.20it/s]\n",
      "Translating hi: 100%|██████████| 2/2 [00:04<00:00,  2.11s/it]\n",
      "Predicting hi: 100%|██████████| 10/10 [00:00<00:00, 9339.35it/s]\n",
      "Translating ja: 100%|██████████| 2/2 [00:00<00:00,  2.92it/s]\n",
      "Translating ja: 100%|██████████| 2/2 [00:05<00:00,  2.52s/it]\n",
      "Predicting ja: 100%|██████████| 10/10 [00:00<00:00, 8224.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance by language:\n",
      "\n",
      "Language: pt\n",
      "Accuracy : 0.8000\n",
      "F1 Score : 0.8889\n",
      "\n",
      "Language: hi\n",
      "Accuracy : 0.8000\n",
      "F1 Score : 0.8889\n",
      "\n",
      "Language: ja\n",
      "Accuracy : 0.6000\n",
      "F1 Score : 0.7143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def translate_to_en(texts, src_lang):\n",
    "    tokenizer.src_lang = lang_codes[src_lang]\n",
    "    outputs = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), 8), desc=f\"Translating {src_lang}\"):\n",
    "        batch = texts[i:i+8]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        bos_token_id = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "        \n",
    "        out = model.generate(**inputs, forced_bos_token_id=bos_token_id)\n",
    "        outputs.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    stopwords = {\"the\",\"a\",\"an\",\"this\",\"that\",\"those\",      # articles\n",
    "    \"is\",\"was\",\"has\",\"have\",\"had\",\"be\",                     # verbs\n",
    "    \"been\",\"do\",\"does\",\"did\",\"are\",\"were\",\n",
    "    \"go\",\"went\",\"goes\",                   \n",
    "    \"in\",\"on\",\"at\",\"of\",\"to\",\"for\",\"with\",                  # prepositions\n",
    "    \"into\",\"from\",\"above\",\"below\",\"before\",\"after\",\n",
    "    \"and\",\"or\",\"but\",                                       # conjunctions\n",
    "    \"there\",\"their\",\"its\",\"it\",                             # possessive/pronouns\n",
    "    \"who\",\"what\",\"when\",\"where\",\"why\",\"how\",\"which\"         # question words\n",
    "    }\n",
    "    return set(lemmatizer.lemmatize(t) for t in tokens if t not in stopwords)\n",
    "\n",
    "def predict_answerable(question, context):\n",
    "    a = preprocess(question)\n",
    "    b = preprocess(context)\n",
    "\n",
    "    return 1 if len(a & b) / len(a) >= 0.5 else 0\n",
    "\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lang in df_test[\"lang\"].unique():\n",
    "    subset = df_test[df_test[\"lang\"] == lang].copy()\n",
    "    \n",
    "    subset[\"question_en\"] = translate_to_en(subset[\"question\"].tolist(), lang)\n",
    "    subset[\"context_en\"] = translate_to_en(subset[\"context\"].tolist(), lang)\n",
    "    \n",
    "    subset[\"pred\"] = [predict_answerable(q, c) for q, c in tqdm(zip(subset[\"question_en\"], subset[\"context_en\"]), total=len(subset), desc=f\"Predicting {lang}\")]\n",
    "    \n",
    "    acc = accuracy_score(subset[\"answerable\"], subset[\"pred\"])\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(subset[\"answerable\"], subset[\"pred\"], average=\"binary\")\n",
    "    \n",
    "    results[lang] = {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "print(\"Performance by language:\")\n",
    "for lang, metrics in results.items():\n",
    "    print(f\"\\nLanguage: {lang}\")\n",
    "    print(f\"Accuracy : {metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score : {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64547f",
   "metadata": {},
   "source": [
    "## Part 3: Trained Answerability Classifier\n",
    "\n",
    "Week 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "276265e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 3300.26 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2748.74 examples/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2496.16 examples/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2637.76 examples/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language: pt\n",
      "Accuracy : 0.8000\n",
      "F1 Score : 0.8889\n",
      "\n",
      "Language: hi\n",
      "Accuracy : 0.8000\n",
      "F1 Score : 0.8889\n",
      "\n",
      "Language: ja\n",
      "Accuracy : 0.8000\n",
      "F1 Score : 0.8889\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Question  Predicted  Actual\n",
      "                             Quem pintou a Mona Lisa?          1       1\n",
      "                        Qual é a capital de Portugal?          1       0\n",
      "    Em que ano o homem pisou a Lua pela primeira vez?          1       1\n",
      "Qual é o elemento mais abundante na crosta terrestre?          1       1\n",
      "                         Onde se originou o Carnaval?          1       1\n",
      "               Qual é a camada mais externa da Terra?          1       1\n",
      "              Qual é a cidade mais populosa do mundo?          1       1\n",
      "            Qual é a distância média da Terra ao Sol?          1       1\n",
      "                           Como funcionam as vacinas?          1       1\n",
      "         Qual é o nome do rio mais longo de Portugal?          1       0\n",
      "                              ताजमहल किसने बनवाया था?          1       1\n",
      "                   भारत के प्रथम प्रधानमंत्री कौन थे?          1       1\n",
      "              मानव शरीर की सबसे बड़ी हड्डी कौन सी है?          1       1\n",
      "                     महात्मा गांधी का जन्म कब हुआ था?          1       1\n",
      "                सौर मंडल का सबसे बड़ा ग्रह कौन सा है?          1       1\n",
      "                             प्रकाश संश्लेषण क्या है?          1       1\n",
      "                     भारत का राष्ट्रीय पशु कौन सा है?          1       0\n",
      "                  भारत में कितने शास्त्रीय नृत्य हैं?          1       0\n",
      "                 पृथ्वी पर सबसे बड़ा जानवर कौन सा है?          1       1\n",
      "                       भारत में पहली ट्रेन कब चली थी?          1       1\n",
      "                                 グレートバリアリーフはどこにありますか？          1       0\n",
      "                                   周期表にはいくつの元素がありますか？          1       1\n",
      "                                      オリンピックの起源は何ですか？          1       1\n",
      "                                  なぜ地震はプレート境界で発生しますか？          1       1\n",
      "                                          エベレスト山の標高は？          1       0\n",
      "                                  ベートーヴェンのフルネームは何ですか？          1       1\n",
      "                                    サハラ砂漠はどの大陸にありますか？          1       1\n",
      "                            ミツバチはどのようにして蜜を蜂蜜に変えるのですか？          1       1\n",
      "                              DNAの二重らせん構造を発見したのは誰ですか？          1       1\n",
      "                                         人間が音を聞く仕組みは？          1       1\n"
     ]
    }
   ],
   "source": [
    "has_labels = \"answerable\" in df_test.columns\n",
    "\n",
    "model_checkpoint = \"chungimungi/week-38-multilingual-distilbert-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenized_input = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    if has_labels:\n",
    "        tokenized_input[\"labels\"] = [int(ans) for ans in examples[\"answerable\"]]\n",
    "    return tokenized_input\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "dataset_test = Dataset.from_pandas(df_test)\n",
    "tokenized_test = dataset_test.map(preprocess_function, batched=True)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results_eval\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    dataloader_drop_last=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "languages = df_test[\"lang\"].unique()\n",
    "\n",
    "results_by_lang = {}\n",
    "\n",
    "for lang in languages:\n",
    "    df_lang = df_test[df_test[\"lang\"] == lang]\n",
    "    dataset_lang = Dataset.from_pandas(df_lang)\n",
    "    tokenized_lang = dataset_lang.map(preprocess_function, batched=True)\n",
    "\n",
    "    predictions = trainer.predict(tokenized_lang)\n",
    "    pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    true_labels = predictions.label_ids\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, average='binary')\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "    results_by_lang[lang] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall\n",
    "    }\n",
    "\n",
    "for lang, metrics in results_by_lang.items():\n",
    "    print(f\"\\nLanguage: {lang}\")\n",
    "    print(f\"Accuracy : {metrics['accuracy']:.4f}\")\n",
    "    print(f\"F1 Score : {metrics['f1']:.4f}\")\n",
    "\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "df_compare = pd.DataFrame({\n",
    "    \"Question\": df_test[\"question\"],\n",
    "    \"Predicted\": pred_labels,\n",
    "    \"Actual\": true_labels\n",
    "})\n",
    "\n",
    "print(df_compare.head(30).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d368c2",
   "metadata": {},
   "source": [
    "## Part 4: Open QA\n",
    "Week 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e997704c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating Model 1: Question in Language + English Context → Answer in Language\n",
      "\n",
      "Language: pt\n",
      "Answerable Examples: 8\n",
      "Unanswerable Examples: 2\n",
      "\n",
      "Answerable Performance:\n",
      "  ROUGE-1: 0.1667\n",
      "  ROUGE-2: 0.15\n",
      "  ROUGE-L: 0.125\n",
      "  BLEU: 0.0\n",
      "\n",
      "Unanswerable Performance:\n",
      "  ROUGE-1: 0.0\n",
      "  ROUGE-2: 0.0\n",
      "  ROUGE-L: 0.0\n",
      "  BLEU: 0\n",
      "\n",
      "Overall Performance:\n",
      "  ROUGE-1: 0.1333\n",
      "  ROUGE-2: 0.12\n",
      "  ROUGE-L: 0.1333\n",
      "  BLEU: 0.0\n",
      "\n",
      "Language: hi\n",
      "Answerable Examples: 8\n",
      "Unanswerable Examples: 2\n",
      "\n",
      "Answerable Performance:\n",
      "  ROUGE-1: 0.2\n",
      "  ROUGE-2: 0.1667\n",
      "  ROUGE-L: 0.2\n",
      "  BLEU: 0.0\n",
      "\n",
      "Unanswerable Performance:\n",
      "  ROUGE-1: 0.0\n",
      "  ROUGE-2: 0.0\n",
      "  ROUGE-L: 0.0\n",
      "  BLEU: 0\n",
      "\n",
      "Overall Performance:\n",
      "  ROUGE-1: 0.16\n",
      "  ROUGE-2: 0.1333\n",
      "  ROUGE-L: 0.16\n",
      "  BLEU: 0.0\n",
      "\n",
      "Language: ja\n",
      "Answerable Examples: 8\n",
      "Unanswerable Examples: 2\n",
      "\n",
      "Answerable Performance:\n",
      "  ROUGE-1: 0.2083\n",
      "  ROUGE-2: 0.0\n",
      "  ROUGE-L: 0.2083\n",
      "  BLEU: 0.0\n",
      "\n",
      "Unanswerable Performance:\n",
      "  ROUGE-1: 0.0\n",
      "  ROUGE-2: 0.0\n",
      "  ROUGE-L: 0.0\n",
      "  BLEU: 0\n",
      "\n",
      "Overall Performance:\n",
      "  ROUGE-1: 0.1667\n",
      "  ROUGE-2: 0.0\n",
      "  ROUGE-L: 0.1667\n",
      "  BLEU: 0.0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "model_checkpoint = \"chungimungi/mbart-te-qc\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "def format_prompt_qc(question, context, lang):\n",
    "    \"\"\"\n",
    "    Improved prompt with clear structure and task instruction.\n",
    "    Uses delimiters and explicit task framing.\n",
    "    \"\"\"\n",
    "    if lang == \"pt\":\n",
    "        prompt = (\n",
    "            f\"Answer the following question based on the given context. \"\n",
    "            f\"Provide a concise and accurate answer in Portuguese.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Context: {context}\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "    elif lang == \"hi\":\n",
    "        prompt = (\n",
    "            f\"Answer the following question based on the given context. \"\n",
    "            f\"Provide a concise and accurate answer in Hindi.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Context: {context}\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "    else: # lang == ja\n",
    "        prompt = (\n",
    "            f\"Answer the following question based on the given context. \"\n",
    "            f\"Provide a concise and accurate answer in Japanese.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Context: {context}\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "    return prompt\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for q, c, a_inlang, a_en, l in zip(examples[\"question\"], examples[\"context\"], examples[\"answer_inlang\"], examples[\"answer\"], examples[\"lang\"]):\n",
    "        input_text = format_prompt_qc(q, c, l)\n",
    "        target_text = a_inlang if (a_inlang is not None and len(a_inlang) > 0) else (a_en if a_en is not None else \"\")\n",
    "        inputs.append(input_text)\n",
    "        targets.append(target_text)\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=False)\n",
    "    labels = tokenizer(targets, max_length=max_length, truncation=True, padding=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def eval_dataset(df_test, lang_code):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    dataset = Dataset.from_pandas(df_test)\n",
    "    model.eval()\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=100,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        num_beams=8,\n",
    "        repetition_penalty=1.6,\n",
    "        length_penalty=1.5,\n",
    "    )\n",
    "\n",
    "    answerable_predictions = []\n",
    "    answerable_references = []\n",
    "    unanswerable_predictions = []\n",
    "    unanswerable_references = []\n",
    "    with torch.no_grad():\n",
    "        for i, ex in enumerate(dataset):            \n",
    "            prompt = format_prompt_qc(ex[\"question\"], ex[\"context\"], ex[\"lang\"])\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).input_ids.to(model.device)\n",
    "            outputs = model.generate(input_ids, **gen_kwargs)\n",
    "            pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if ex[\"answerable\"]:\n",
    "                if ex[\"answer_inlang\"] and len(ex[\"answer_inlang\"]) > 0:\n",
    "                    ref_text = ex[\"answer_inlang\"]\n",
    "                else:\n",
    "                    ref_text = ex[\"answer\"]\n",
    "                answerable_predictions.append(pred_text.strip())\n",
    "                answerable_references.append(ref_text.strip())\n",
    "            else:\n",
    "                unanswerable_predictions.append(pred_text.strip())\n",
    "                unanswerable_references.append(\"\")\n",
    "\n",
    "    answerable_rouge = rouge.compute(predictions=answerable_predictions, references=answerable_references, use_stemmer=True) if answerable_predictions else {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    try:\n",
    "        answerable_bleu = bleu.compute(predictions=answerable_predictions, references=[[r] for r in answerable_references]) if answerable_predictions else {\"bleu\": 0}\n",
    "    except ZeroDivisionError:\n",
    "        answerable_bleu = {\"bleu\": 0}\n",
    "    unanswerable_rouge = rouge.compute(predictions=unanswerable_predictions, references=unanswerable_references, use_stemmer=True) if unanswerable_predictions else {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    try:\n",
    "        unanswerable_bleu = bleu.compute(predictions=unanswerable_predictions, references=[[r] for r in unanswerable_references]) if unanswerable_predictions else {\"bleu\": 0}\n",
    "    except ZeroDivisionError:\n",
    "        unanswerable_bleu = {\"bleu\": 0}\n",
    "    all_predictions = answerable_predictions + unanswerable_predictions\n",
    "    all_references = answerable_references + unanswerable_references\n",
    "    overall_rouge = rouge.compute(predictions=all_predictions, references=all_references, use_stemmer=True) if all_predictions else {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    try:\n",
    "        overall_bleu = bleu.compute(predictions=all_predictions, references=[[r] for r in all_references]) if all_predictions else {\"bleu\": 0}\n",
    "    except ZeroDivisionError:\n",
    "        overall_bleu = {\"bleu\": 0}\n",
    "    results = {\n",
    "        \"answerable_rouge1\": round(answerable_rouge.get(\"rouge1\", 0), 4),\n",
    "        \"answerable_rouge2\": round(answerable_rouge.get(\"rouge2\", 0), 4),\n",
    "        \"answerable_rougeL\": round(answerable_rouge.get(\"rougeL\", 0), 4),\n",
    "        \"answerable_bleu\": round(answerable_bleu.get(\"bleu\", 0), 4),\n",
    "        \"unanswerable_rouge1\": round(unanswerable_rouge.get(\"rouge1\", 0), 4),\n",
    "        \"unanswerable_rouge2\": round(unanswerable_rouge.get(\"rouge2\", 0), 4),\n",
    "        \"unanswerable_rougeL\": round(unanswerable_rouge.get(\"rougeL\", 0), 4),\n",
    "        \"unanswerable_bleu\": round(unanswerable_bleu.get(\"bleu\", 0), 4),\n",
    "        \"overall_rouge1\": round(overall_rouge.get(\"rouge1\", 0), 4),\n",
    "        \"overall_rouge2\": round(overall_rouge.get(\"rouge2\", 0), 4),\n",
    "        \"overall_rougeL\": round(overall_rouge.get(\"rougeL\", 0), 4),\n",
    "        \"overall_bleu\": round(overall_bleu.get(\"bleu\", 0), 4),\n",
    "        \"answerable_count\": len(answerable_predictions),\n",
    "        \"unanswerable_count\" : len(unanswerable_predictions)\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "# def prompt_fn_qc(example):\n",
    "#     return format_prompt_qc(example[\"question\"], example[\"context\"])\n",
    "\n",
    "languages = df_test[\"lang\"].unique()\n",
    "\n",
    "results_by_lang = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating Model 1: Question in Language + English Context → Answer in Language\")\n",
    "\n",
    "languages = df_test[\"lang\"].unique()\n",
    "results_by_lang = {}\n",
    "\n",
    "for lang in languages:\n",
    "    df_lang = df_test[df_test[\"lang\"] == lang]\n",
    "    results_by_lang[lang] = eval_dataset(df_lang, lang)\n",
    "\n",
    "\n",
    "for lang, results in results_by_lang.items():\n",
    "    print(f\"\\nLanguage: {lang}\")\n",
    "    print(f\"Answerable Examples: {results['answerable_count']}\")\n",
    "    print(f\"Unanswerable Examples: {results['unanswerable_count']}\")\n",
    "    \n",
    "    print(\"\\nAnswerable Performance:\")\n",
    "    print(f\"  ROUGE-1: {results['answerable_rouge1']}\")\n",
    "    print(f\"  ROUGE-2: {results['answerable_rouge2']}\")\n",
    "    print(f\"  ROUGE-L: {results['answerable_rougeL']}\")\n",
    "    print(f\"  BLEU: {results['answerable_bleu']}\")\n",
    "\n",
    "    print(\"\\nUnanswerable Performance:\")\n",
    "    print(f\"  ROUGE-1: {results['unanswerable_rouge1']}\")\n",
    "    print(f\"  ROUGE-2: {results['unanswerable_rouge2']}\")\n",
    "    print(f\"  ROUGE-L: {results['unanswerable_rougeL']}\")\n",
    "    print(f\"  BLEU: {results['unanswerable_bleu']}\")\n",
    "\n",
    "    print(\"\\nOverall Performance:\")\n",
    "    print(f\"  ROUGE-1: {results['overall_rouge1']}\")\n",
    "    print(f\"  ROUGE-2: {results['overall_rouge2']}\")\n",
    "    print(f\"  ROUGE-L: {results['overall_rougeL']}\")\n",
    "    print(f\"  BLEU: {results['overall_bleu']}\")\n",
    "    \n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8417598",
   "metadata": {},
   "source": [
    "## Part 5: Sequence Labeler \n",
    "\n",
    "Week 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752f60af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:00<00:00, 2932.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating per language...\n",
      "\n",
      "=================================================================\n",
      "Evaluating on pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 30/30 [00:00<00:00, 18867.76 examples/s]\n",
      "Filter: 100%|██████████| 30/30 [00:00<00:00, 1958.61 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for pt:\n",
      "Token-level Accuracy:  0.8775\n",
      "F1 Score:              0.5394 (n=10)\n",
      "=================================================================\n",
      "\n",
      "\n",
      "=================================================================\n",
      "Evaluating on hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 30/30 [00:00<00:00, 16582.65 examples/s]\n",
      "Filter: 100%|██████████| 30/30 [00:00<00:00, 1917.28 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for hi:\n",
      "Token-level Accuracy:  0.8975\n",
      "F1 Score:              0.6450 (n=10)\n",
      "=================================================================\n",
      "\n",
      "\n",
      "=================================================================\n",
      "Evaluating on ja\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 30/30 [00:00<00:00, 17647.84 examples/s]\n",
      "Filter: 100%|██████████| 30/30 [00:00<00:00, 1930.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ja:\n",
      "Token-level Accuracy:  0.8440\n",
      "F1 Score:              0.5310 (n=10)\n",
      "=================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "CROSS-LANGUAGE COMPARISON\n",
      "==================================================\n",
      "Language     Acc       F1      \n",
      "--------------------------------------------------\n",
      "PT          0.8775   0.5394\n",
      "HI          0.8975   0.6450\n",
      "JA          0.8440   0.5310\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "model_checkpoint = \"chungimungi/week-40-multilingual-distilbert-sequence-label\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "label_list = [\"O\", \"ANS\"]\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "max_length = 384\n",
    "doc_stride = 128\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "def create_token_labels(examples):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized[\"overflow_to_sample_mapping\"]\n",
    "    offsets_mapping = tokenized[\"offset_mapping\"]\n",
    "\n",
    "    labels = []\n",
    "    langs = []\n",
    "    for i, offsets in enumerate(offsets_mapping):\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_idx = sample_mapping[i]\n",
    "        context = examples[\"context\"][sample_idx]\n",
    "        answer = (examples.get(\"answer\", [\"\"])[sample_idx] or \"\").strip()\n",
    "        answerable = examples.get(\"answerable\", [True])[sample_idx]\n",
    "\n",
    "        if not answerable or not answer:\n",
    "            answer_start, answer_end = -1, -1\n",
    "        else:\n",
    "            match = re.search(re.escape(answer), context, flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                answer_start, answer_end = match.start(), match.end()\n",
    "            else:\n",
    "                answer_start, answer_end = -1, -1\n",
    "\n",
    "        langs.append(examples.get(\"lang\", [\"unk\"])[sample_idx])\n",
    "\n",
    "        example_labels = []\n",
    "        context_id = 1 if pad_on_right else 0\n",
    "\n",
    "        for idx, offset in enumerate(offsets):\n",
    "            if sequence_ids[idx] is None:\n",
    "                example_labels.append(-100)\n",
    "            elif sequence_ids[idx] != context_id:\n",
    "                example_labels.append(-100)\n",
    "            else:\n",
    "                if answer_start == -1 or offset is None:\n",
    "                    example_labels.append(label_to_id[\"O\"])\n",
    "                else:\n",
    "                    start, end = offset\n",
    "                    if start >= answer_end or end <= answer_start:\n",
    "                        example_labels.append(label_to_id[\"O\"])\n",
    "                    else:\n",
    "                        example_labels.append(label_to_id[\"ANS\"])\n",
    "        labels.append(example_labels)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    tokenized[\"lang\"] = langs\n",
    "    tokenized[\"offset_mapping\"] = offsets_mapping\n",
    "    return tokenized\n",
    "\n",
    "def extract_answer_from_predictions(input_ids, predictions, offset_mapping, sequence_ids, context_id=1):\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, (pred, sid) in enumerate(zip(predictions, sequence_ids)):\n",
    "        if sid == context_id and pred == label_to_id[\"ANS\"]:\n",
    "            if start is None:\n",
    "                start = i\n",
    "        else:\n",
    "            if start is not None:\n",
    "                spans.append((start, i - 1))\n",
    "                start = None\n",
    "    if start is not None:\n",
    "        spans.append((start, len(predictions) - 1))\n",
    "    best_text = \"\"\n",
    "    best_len = -1\n",
    "    for s, e in spans:\n",
    "        if offset_mapping[s] is not None and offset_mapping[e] is not None:\n",
    "            tokens = input_ids[s:e+1]\n",
    "            text = tokenizer.decode(tokens, skip_special_tokens=True).strip()\n",
    "            if len(text) > best_len:\n",
    "                best_text = text\n",
    "                best_len = len(text)\n",
    "    return best_text\n",
    "\n",
    "def compute_token_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    true_labels, pred_labels = [], []\n",
    "\n",
    "    for p, l in zip(preds, labels):\n",
    "        for pi, li in zip(p, l):\n",
    "            if li != -100:\n",
    "                true_labels.append(li)\n",
    "                pred_labels.append(pi)\n",
    "    \n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc\n",
    "    }\n",
    "\n",
    "def compute_f1(trainer, eval_dataset, original_dataset):\n",
    "    predictions = trainer.predict(eval_dataset)\n",
    "    pred_logits = predictions.predictions\n",
    "    pred_labels = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    sample_to_predictions = defaultdict(list)\n",
    "\n",
    "    for i in range(len(eval_dataset)):\n",
    "        input_ids = eval_dataset[i][\"input_ids\"]\n",
    "        offset_mapping = eval_dataset[i][\"offset_mapping\"]\n",
    "\n",
    "        sequence_ids = [\n",
    "            0 if idx < input_ids.index(tokenizer.sep_token_id)\n",
    "            else 1 if idx > input_ids.index(tokenizer.sep_token_id)\n",
    "            else None\n",
    "            for idx in range(len(input_ids))\n",
    "        ]\n",
    "\n",
    "        pred_answer = extract_answer_from_predictions(\n",
    "            input_ids, pred_labels[i], offset_mapping, sequence_ids, \n",
    "            context_id=1 if pad_on_right else 0\n",
    "        )\n",
    "        sample_to_predictions[i].append(pred_answer)\n",
    "\n",
    "    for sample_idx, pred_answers in sample_to_predictions.items():\n",
    "        true_answer = original_dataset[sample_idx][\"answer\"].strip().lower()\n",
    "        \n",
    "        best_f1 = 0.0\n",
    "        for pred_answer in pred_answers:\n",
    "            pred_answer = pred_answer.strip().lower()\n",
    "            if pred_answer == \"\":\n",
    "                f1_scores.append(0.0)\n",
    "                continue\n",
    "            true_chars = set(range(len(true_answer)))\n",
    "            pred_chars = set(range(len(pred_answer)))\n",
    "            common = min(len(true_chars), len(pred_chars))\n",
    "            precision = common / len(pred_chars) if pred_chars else 0\n",
    "            recall = common / len(true_chars) if true_chars else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "        f1_scores.append(best_f1)\n",
    "\n",
    "    avg_f1 = np.mean(f1_scores) if f1_scores else 0.0\n",
    "    return avg_f1, len(f1_scores)\n",
    "\n",
    "tokenized_test = test_dataset.map(\n",
    "    create_token_labels, \n",
    "    batched=True, \n",
    "    remove_columns=test_dataset.column_names\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, \n",
    "    compute_metrics=compute_token_metrics\n",
    ")\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "token_metrics = trainer.evaluate(tokenized_test)\n",
    "f1_score, f1_count = compute_f1(trainer, tokenized_test, test_dataset)\n",
    "\n",
    "print(f\"Evaluating per language...\")\n",
    "results_by_lang = {}\n",
    "\n",
    "for lang in languages:\n",
    "    print(f\"\\n{'='*65}\")\n",
    "    print(f\"Evaluating on {lang.lower()}\")\n",
    "    \n",
    "    lang_data = test_dataset.filter(lambda ex: ex[\"lang\"] == lang)\n",
    "    tokenized_lang_data = tokenized_test.filter(lambda ex: ex[\"lang\"] == lang)\n",
    "    token_metrics = trainer.evaluate(tokenized_lang_data)\n",
    "    f1_score, f1_count = compute_f1(trainer, tokenized_lang_data, lang_data)\n",
    "    \n",
    "    results_by_lang[lang] = {\n",
    "        **token_metrics,\n",
    "        \"f1\": f1_score,\n",
    "        \"f1_count\": f1_count\n",
    "    }\n",
    "    \n",
    "    print(f\"Results for {lang.lower()}:\")\n",
    "    print(f\"Token-level Accuracy:  {token_metrics['eval_accuracy']:.4f}\")\n",
    "    print(f\"F1 Score:              {f1_score:.4f} (n={f1_count})\")\n",
    "    print(f\"{'='*65}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CROSS-LANGUAGE COMPARISON\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"{'Language':<12} {'Acc':<8}  {'F1':<8}\")\n",
    "print(f\"{'-'*50}\")\n",
    "lang_names = {\"pt\": \"Portuguese\", \"hi\": \"Hindi\", \"ja\": \"Japanese\"}\n",
    "for lang in languages:\n",
    "    metrics = results_by_lang[lang]\n",
    "    print(f\"{lang.upper():<12}\"\n",
    "        f\"{metrics['eval_accuracy']:.4f}   \"\n",
    "        f\"{metrics['f1']:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
