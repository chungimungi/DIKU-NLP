{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e2cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "full_dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "\n",
    "model_checkpoint = \"google/mt5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "bleu_metric = load(\"bleu\")\n",
    "rouge_metric = load(\"rouge\")\n",
    "\n",
    "train_dataset = full_dataset[\"train\"]\n",
    "val_dataset = full_dataset[\"validation\"]\n",
    "\n",
    "te_train = train_dataset.filter(lambda ex: ex[\"lang\"] == \"te\" and ex[\"answer_inlang\"] != \"\")\n",
    "te_val = val_dataset.filter(lambda ex: ex[\"lang\"] == \"te\" and ex[\"answer_inlang\"] != \"\")\n",
    "\n",
    "print(f\"Telugu train samples with answer_inlang: {len(te_train)}\")\n",
    "print(f\"Telugu val samples with answer_inlang: {len(te_val)}\")\n",
    "\n",
    "def prepare_data_model1(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for q, c, a in zip(examples[\"question\"], examples[\"context\"], examples[\"answer_inlang\"]):\n",
    "        q_str = str(q) if q is not None else \"\"\n",
    "        c_str = str(c) if c is not None else \"\"\n",
    "        a_str = str(a) if a is not None else \"\"\n",
    "        \n",
    "        input_text = f\"Question: {q_str} Context: {c_str}\"\n",
    "        inputs.append(input_text)\n",
    "        targets.append(a_str)\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    labels_list = []\n",
    "    for label_ids in labels[\"input_ids\"]:\n",
    "        label_ids = [l if l != tokenizer.pad_token_id else -100 for l in label_ids]\n",
    "        labels_list.append(label_ids)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels_list\n",
    "    return model_inputs\n",
    "\n",
    "def prepare_data_model2(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for q, a in zip(examples[\"question\"], examples[\"answer_inlang\"]):\n",
    "        q_str = str(q) if q is not None else \"\"\n",
    "        a_str = str(a) if a is not None else \"\"\n",
    "        \n",
    "        input_text = f\"Question: {q_str}\"\n",
    "        inputs.append(input_text)\n",
    "        targets.append(a_str)\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    labels_list = []\n",
    "    for label_ids in labels[\"input_ids\"]:\n",
    "        label_ids = [l if l != tokenizer.pad_token_id else -100 for l in label_ids]\n",
    "        labels_list.append(label_ids)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels_list\n",
    "    return model_inputs\n",
    "\n",
    "def prepare_data_model3(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for q, a in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        q_str = str(q) if q is not None else \"\"\n",
    "        a_str = str(a) if a is not None else \"\"\n",
    "        \n",
    "        input_text = f\"Translate to English. Question: {q_str}\"\n",
    "        inputs.append(input_text)\n",
    "        targets.append(a_str)\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    labels_list = []\n",
    "    for label_ids in labels[\"input_ids\"]:\n",
    "        label_ids = [l if l != tokenizer.pad_token_id else -100 for l in label_ids]\n",
    "        labels_list.append(label_ids)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels_list\n",
    "    return model_inputs\n",
    "\n",
    "def compute_metrics_model1(model):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for i in range(len(te_val)):\n",
    "        q = str(te_val[i][\"question\"]) if te_val[i][\"question\"] is not None else \"\"\n",
    "        c = str(te_val[i][\"context\"]) if te_val[i][\"context\"] is not None else \"\"\n",
    "        input_text = f\"Question: {q} Context: {c}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "        \n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        ref = te_val[i][\"answer_inlang\"]\n",
    "        ref = str(ref) if ref is not None else \"\"\n",
    "        \n",
    "        if i < 3:\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Question: {q[:100]}...\")\n",
    "            print(f\"Prediction: {pred[:100]}...\")\n",
    "            print(f\"Reference: {ref[:100]}...\")\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "    \n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=[[r] for r in references])\n",
    "    rouge = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu[\"bleu\"],\n",
    "        \"rouge1\": rouge[\"rouge1\"],\n",
    "        \"rouge2\": rouge[\"rouge2\"],\n",
    "        \"rougeL\": rouge[\"rougeL\"]\n",
    "    }\n",
    "\n",
    "def compute_metrics_model2(model):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for i in range(len(te_val)):\n",
    "        q = str(te_val[i][\"question\"]) if te_val[i][\"question\"] is not None else \"\"\n",
    "        input_text = f\"Question: {q}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "        \n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        ref = te_val[i][\"answer_inlang\"]\n",
    "        ref = str(ref) if ref is not None else \"\"\n",
    "        \n",
    "        if i < 3:\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Question: {q[:100]}...\")\n",
    "            print(f\"Prediction: {pred[:100]}...\")\n",
    "            print(f\"Reference: {ref[:100]}...\")\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "    \n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=[[r] for r in references])\n",
    "    rouge = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu[\"bleu\"],\n",
    "        \"rouge1\": rouge[\"rouge1\"],\n",
    "        \"rouge2\": rouge[\"rouge2\"],\n",
    "        \"rougeL\": rouge[\"rougeL\"]\n",
    "    }\n",
    "\n",
    "def compute_metrics_model3(model):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for i in range(len(te_val)):\n",
    "        q = str(te_val[i][\"question\"]) if te_val[i][\"question\"] is not None else \"\"\n",
    "        input_text = f\"Translate to English. Question: {q}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "        \n",
    "        pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        ref = te_val[i][\"answer\"]\n",
    "        ref = str(ref) if ref is not None else \"\"\n",
    "        \n",
    "        if i < 3:\n",
    "            print(f\"\\nExample {i+1}:\")\n",
    "            print(f\"Question: {q[:100]}...\")\n",
    "            print(f\"Prediction: {pred[:100]}...\")\n",
    "            print(f\"Reference: {ref[:100]}...\")\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        references.append(ref)\n",
    "    \n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=[[r] for r in references])\n",
    "    rouge = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu[\"bleu\"],\n",
    "        \"rouge1\": rouge[\"rouge1\"],\n",
    "        \"rouge2\": rouge[\"rouge2\"],\n",
    "        \"rougeL\": rouge[\"rougeL\"]\n",
    "    }\n",
    "\n",
    "tokenized_train_m1 = te_train.map(prepare_data_model1, batched=True, remove_columns=te_train.column_names)\n",
    "tokenized_val_m1 = te_val.map(prepare_data_model1, batched=True, remove_columns=te_val.column_names)\n",
    "\n",
    "tokenized_train_m2 = te_train.map(prepare_data_model2, batched=True, remove_columns=te_train.column_names)\n",
    "tokenized_val_m2 = te_val.map(prepare_data_model2, batched=True, remove_columns=te_val.column_names)\n",
    "\n",
    "tokenized_train_m3 = te_train.map(prepare_data_model3, batched=True, remove_columns=te_train.column_names)\n",
    "tokenized_val_m3 = te_val.map(prepare_data_model3, batched=True, remove_columns=te_val.column_names)\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 1: Telugu Question + English Context -> Telugu Answer\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_m1 = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, device_map=\"auto\")\n",
    "\n",
    "lora_config_m1 = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model_m1 = get_peft_model(model_m1, lora_config_m1)\n",
    "model_m1.print_trainable_parameters()\n",
    "\n",
    "args_m1 = TrainingArguments(\n",
    "    output_dir=\"mt5-base-te-qa-context\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=[],\n",
    "    push_to_hub=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "data_collator_m1 = DataCollatorForSeq2Seq(tokenizer, model=model_m1)\n",
    "\n",
    "trainer_m1 = Trainer(\n",
    "    model=model_m1,\n",
    "    args=args_m1,\n",
    "    train_dataset=tokenized_train_m1,\n",
    "    eval_dataset=tokenized_val_m1,\n",
    "    data_collator=data_collator_m1,\n",
    ")\n",
    "\n",
    "trainer_m1.train()\n",
    "eval_results = trainer_m1.evaluate()\n",
    "\n",
    "print(f\"Computing BLEU and ROUGE...\")\n",
    "metrics = compute_metrics_model1(model_m1)\n",
    "\n",
    "results[\"model1\"] = {\n",
    "    \"eval_loss\": eval_results[\"eval_loss\"],\n",
    "    **metrics\n",
    "}\n",
    "\n",
    "print(f\"Model 1 - Loss: {results['model1']['eval_loss']:.4f}, BLEU: {results['model1']['bleu']:.4f}, ROUGE-L: {results['model1']['rougeL']:.4f}\")\n",
    "\n",
    "del model_m1, trainer_m1\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2: Telugu Question -> Telugu Answer\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_m2 = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, device_map=\"auto\")\n",
    "\n",
    "lora_config_m2 = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model_m2 = get_peft_model(model_m2, lora_config_m2)\n",
    "model_m2.print_trainable_parameters()\n",
    "\n",
    "args_m2 = TrainingArguments(\n",
    "    output_dir=\"mt5-base-te-qa-no-context\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=[],\n",
    "    push_to_hub=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "data_collator_m2 = DataCollatorForSeq2Seq(tokenizer, model=model_m2)\n",
    "\n",
    "trainer_m2 = Trainer(\n",
    "    model=model_m2,\n",
    "    args=args_m2,\n",
    "    train_dataset=tokenized_train_m2,\n",
    "    eval_dataset=tokenized_val_m2,\n",
    "    data_collator=data_collator_m2,\n",
    ")\n",
    "\n",
    "trainer_m2.train()\n",
    "eval_results = trainer_m2.evaluate()\n",
    "\n",
    "print(f\"Computing BLEU and ROUGE...\")\n",
    "metrics = compute_metrics_model2(model_m2)\n",
    "\n",
    "results[\"model2\"] = {\n",
    "    \"eval_loss\": eval_results[\"eval_loss\"],\n",
    "    **metrics\n",
    "}\n",
    "\n",
    "print(f\"Model 2 - Loss: {results['model2']['eval_loss']:.4f}, BLEU: {results['model2']['bleu']:.4f}, ROUGE-L: {results['model2']['rougeL']:.4f}\")\n",
    "\n",
    "del model_m2, trainer_m2\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 3: Telugu Question -> English Answer\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_m3 = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, device_map=\"auto\")\n",
    "\n",
    "lora_config_m3 = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model_m3 = get_peft_model(model_m3, lora_config_m3)\n",
    "model_m3.print_trainable_parameters()\n",
    "\n",
    "args_m3 = TrainingArguments(\n",
    "    output_dir=\"mt5-base-te-qa-en-answer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=[],\n",
    "    push_to_hub=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "data_collator_m3 = DataCollatorForSeq2Seq(tokenizer, model=model_m3)\n",
    "\n",
    "trainer_m3 = Trainer(\n",
    "    model=model_m3,\n",
    "    args=args_m3,\n",
    "    train_dataset=tokenized_train_m3,\n",
    "    eval_dataset=tokenized_val_m3,\n",
    "    data_collator=data_collator_m3,\n",
    ")\n",
    "\n",
    "trainer_m3.train()\n",
    "eval_results = trainer_m3.evaluate()\n",
    "\n",
    "print(f\"Computing BLEU and ROUGE...\")\n",
    "metrics = compute_metrics_model3(model_m3)\n",
    "\n",
    "results[\"model3\"] = {\n",
    "    \"eval_loss\": eval_results[\"eval_loss\"],\n",
    "    **metrics\n",
    "}\n",
    "\n",
    "print(f\"Model 3 - Loss: {results['model3']['eval_loss']:.4f}, BLEU: {results['model3']['bleu']:.4f}, ROUGE-L: {results['model3']['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<30} {'Loss':<10} {'BLEU':<10} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'M1: Q+C->A_te':<30} {results['model1']['eval_loss']:<10.4f} {results['model1']['bleu']:<10.4f} {results['model1']['rouge1']:<10.4f} {results['model1']['rouge2']:<10.4f} {results['model1']['rougeL']:<10.4f}\")\n",
    "print(f\"{'M2: Q->A_te':<30} {results['model2']['eval_loss']:<10.4f} {results['model2']['bleu']:<10.4f} {results['model2']['rouge1']:<10.4f} {results['model2']['rouge2']:<10.4f} {results['model2']['rougeL']:<10.4f}\")\n",
    "print(f\"{'M3: Q->A_en':<30} {results['model3']['eval_loss']:<10.4f} {results['model3']['bleu']:<10.4f} {results['model3']['rouge1']:<10.4f} {results['model3']['rouge2']:<10.4f} {results['model3']['rougeL']:<10.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
