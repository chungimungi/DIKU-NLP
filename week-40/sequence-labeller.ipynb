{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7be826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training multilingual sequence labeler\n",
      "============================================================\n",
      "Training samples: 15343\n",
      "Validation samples: 3011\n",
      "Computed class weights: {0: 5.555032585083273, 1: 82.41834980661795}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f568de8a844f41c4823fbf6224f4b0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aarus\\AppData\\Local\\Temp\\ipykernel_26952\\3907673728.py:232: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9880' max='24700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9880/24700 54:05 < 1:21:09, 3.04 it/s, Epoch 20/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.690200</td>\n",
       "      <td>2.664846</td>\n",
       "      <td>0.500284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.565000</td>\n",
       "      <td>2.639022</td>\n",
       "      <td>0.606317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.535900</td>\n",
       "      <td>2.619267</td>\n",
       "      <td>0.726457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.555300</td>\n",
       "      <td>2.607516</td>\n",
       "      <td>0.717664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.488800</td>\n",
       "      <td>2.614565</td>\n",
       "      <td>0.851920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.521500</td>\n",
       "      <td>2.612075</td>\n",
       "      <td>0.847645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.448500</td>\n",
       "      <td>2.636699</td>\n",
       "      <td>0.900989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.398700</td>\n",
       "      <td>2.637150</td>\n",
       "      <td>0.886161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.388600</td>\n",
       "      <td>2.636163</td>\n",
       "      <td>0.927144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.382500</td>\n",
       "      <td>2.644253</td>\n",
       "      <td>0.916865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.350100</td>\n",
       "      <td>2.656905</td>\n",
       "      <td>0.931604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.231900</td>\n",
       "      <td>2.659843</td>\n",
       "      <td>0.940231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.378600</td>\n",
       "      <td>2.647770</td>\n",
       "      <td>0.915637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.318700</td>\n",
       "      <td>2.660389</td>\n",
       "      <td>0.947395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.359400</td>\n",
       "      <td>2.669983</td>\n",
       "      <td>0.948512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.315100</td>\n",
       "      <td>2.660764</td>\n",
       "      <td>0.950858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.318500</td>\n",
       "      <td>2.669019</td>\n",
       "      <td>0.949192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.298600</td>\n",
       "      <td>2.665378</td>\n",
       "      <td>0.954106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.273800</td>\n",
       "      <td>2.681796</td>\n",
       "      <td>0.946441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.311900</td>\n",
       "      <td>2.678596</td>\n",
       "      <td>0.951476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating per language...\n",
      "\n",
      "============================================================\n",
      "Evaluating on ar\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e352e0b63749579a5b2b69d1347977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating token-level metrics...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing F1...\n",
      "Prediction distribution: {0: 147919, 1: 19505}\n",
      "\n",
      "============================================================\n",
      "Results for ar:\n",
      "============================================================\n",
      "Token-level Accuracy:  0.9488\n",
      "F1 Score:              0.4709 (n=126)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Evaluating on ko\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be39f8c0378241f4b2e3c6d6bb420bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating token-level metrics...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing F1...\n",
      "Prediction distribution: {0: 120693, 1: 18315}\n",
      "\n",
      "============================================================\n",
      "Results for ko:\n",
      "============================================================\n",
      "Token-level Accuracy:  0.9504\n",
      "F1 Score:              0.5195 (n=105)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Evaluating on te\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa525bf32f20402f81da60a155a05548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating token-level metrics...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing F1...\n",
      "Prediction distribution: {0: 128793, 1: 20199}\n",
      "\n",
      "============================================================\n",
      "Results for te:\n",
      "============================================================\n",
      "Token-level Accuracy:  0.9563\n",
      "F1 Score:              0.7060 (n=285)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "CROSS-LANGUAGE COMPARISON\n",
      "============================================================\n",
      "Language     Acc       F1      \n",
      "------------------------------------------------------------\n",
      "ARABIC       0.9488   0.4709\n",
      "KOREAN       0.9504   0.5195\n",
      "TELUGU       0.9563   0.7060\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ar': {'eval_loss': 2.7534103393554688,\n",
       "  'eval_accuracy': 0.9487802150400204,\n",
       "  'eval_runtime': 1.5483,\n",
       "  'eval_samples_per_second': 281.594,\n",
       "  'eval_steps_per_second': 35.522,\n",
       "  'epoch': 20.0,\n",
       "  'f1': 0.47086206945954434,\n",
       "  'f1_count': 126},\n",
       " 'ko': {'eval_loss': 2.6023454666137695,\n",
       "  'eval_accuracy': 0.9503908725966618,\n",
       "  'eval_runtime': 1.2988,\n",
       "  'eval_samples_per_second': 278.718,\n",
       "  'eval_steps_per_second': 35.417,\n",
       "  'epoch': 20.0,\n",
       "  'f1': 0.5195303797508997,\n",
       "  'f1_count': 105},\n",
       " 'te': {'eval_loss': 2.7963624000549316,\n",
       "  'eval_accuracy': 0.9562619752656332,\n",
       "  'eval_runtime': 1.4434,\n",
       "  'eval_samples_per_second': 268.81,\n",
       "  'eval_steps_per_second': 33.948,\n",
       "  'epoch': 20.0,\n",
       "  'f1': 0.7059633890748965,\n",
       "  'f1_count': 285}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import load_dataset\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "full_dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "\n",
    "model_checkpoint = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "label_list = [\"O\", \"ANS\"]\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "max_length = 384\n",
    "doc_stride = 128\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "languages = [\"ar\", \"ko\", \"te\"]\n",
    "\n",
    "train_dataset = full_dataset[\"train\"]\n",
    "val_dataset = full_dataset[\"validation\"]\n",
    "\n",
    "def analyze_class_distribution(dataset, lang):\n",
    "    labels = []\n",
    "    for example in dataset:\n",
    "        if example[\"answer_start\"] == -1:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    \n",
    "    counter = Counter(labels)\n",
    "    total = len(labels)\n",
    "    \n",
    "    print(f\"\\n--- Class Distribution for {lang.lower()} ---\")\n",
    "    print(f\"Class 0 (Not Answerable): {counter[0]} ({counter[0]/total:.2%})\")\n",
    "    print(f\"Class 1 (Answerable): {counter[1]} ({counter[1]/total:.2%})\")\n",
    "    print(f\"Imbalance Ratio: {max(counter.values()) / min(counter.values()):.2f}:1\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight('balanced', classes=unique_labels, y=labels)\n",
    "    class_weights[1] *= 150\n",
    "    class_weight_dict = dict(zip(unique_labels, class_weights))\n",
    "    \n",
    "    print(f\"Computed class weights: {class_weight_dict}\")\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "def create_token_labels(examples):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized[\"overflow_to_sample_mapping\"]\n",
    "    offsets_mapping = tokenized[\"offset_mapping\"]\n",
    "\n",
    "    labels = []\n",
    "    langs = [] \n",
    "    for i, offsets in enumerate(offsets_mapping):\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_idx = sample_mapping[i]\n",
    "        answer_start = examples[\"answer_start\"][sample_idx]\n",
    "        answer_text = examples[\"answer\"][sample_idx]\n",
    "        answer_end = -1 if answer_start == -1 else answer_start + len(answer_text)\n",
    "        \n",
    "        langs.append(examples[\"lang\"][sample_idx])\n",
    "\n",
    "        example_labels = []\n",
    "        context_id = 1 if pad_on_right else 0\n",
    "        \n",
    "        for idx, offset in enumerate(offsets):\n",
    "            if sequence_ids[idx] is None:\n",
    "                example_labels.append(-100)\n",
    "            elif sequence_ids[idx] != context_id:\n",
    "                example_labels.append(-100)\n",
    "            else:\n",
    "                if answer_start == -1 or offset is None:\n",
    "                    example_labels.append(label_to_id[\"O\"])\n",
    "                else:\n",
    "                    start, end = offset\n",
    "                    if start >= answer_end or end <= answer_start:\n",
    "                        example_labels.append(label_to_id[\"O\"])\n",
    "                    else:\n",
    "                        example_labels.append(label_to_id[\"ANS\"])\n",
    "        labels.append(example_labels)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    tokenized[\"lang\"] = langs  \n",
    "    tokenized[\"offset_mapping\"] = offsets_mapping\n",
    "    return tokenized\n",
    "\n",
    "def extract_answer_from_predictions(input_ids, predictions, offset_mapping, sequence_ids, context_id=1):\n",
    "    spans = []\n",
    "    start = None\n",
    "    for i, (pred, sid) in enumerate(zip(predictions, sequence_ids)):\n",
    "        if sid == context_id and pred == label_to_id[\"ANS\"]:\n",
    "            if start is None:\n",
    "                start = i\n",
    "        else:\n",
    "            if start is not None:\n",
    "                spans.append((start, i-1))\n",
    "                start = None\n",
    "    if start is not None:\n",
    "        spans.append((start, len(predictions)-1))\n",
    "    best_text = \"\"\n",
    "    best_len = -1\n",
    "    for s, e in spans:\n",
    "        if offset_mapping[s] is not None and offset_mapping[e] is not None:\n",
    "            tokens = input_ids[s:e+1]\n",
    "            text = tokenizer.decode(tokens, skip_special_tokens=True).strip()\n",
    "            if len(text) > best_len:\n",
    "                best_text = text\n",
    "                best_len = len(text)\n",
    "    return best_text\n",
    "\n",
    "def compute_token_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for p, l in zip(preds, labels):\n",
    "        for pi, li in zip(p, l):\n",
    "            if li != -100:\n",
    "                true_labels.append(li)\n",
    "                pred_labels.append(pi)\n",
    "    \n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc\n",
    "    }\n",
    "\n",
    "def compute_f1(trainer, eval_dataset, original_dataset):\n",
    "    predictions = trainer.predict(eval_dataset)\n",
    "    pred_logits = predictions.predictions\n",
    "    pred_labels = np.argmax(pred_logits, axis=-1)\n",
    "    \n",
    "    f1_scores = []\n",
    "    \n",
    "    sample_to_predictions = defaultdict(list)\n",
    "    \n",
    "    for i in range(len(eval_dataset)):\n",
    "        input_ids = eval_dataset[i][\"input_ids\"]\n",
    "        offset_mapping = eval_dataset[i].get(\"offset_mapping\", None)\n",
    "        \n",
    "        sequence_ids = []\n",
    "        for idx in range(len(input_ids)):\n",
    "            sid = None\n",
    "            if input_ids[idx] == tokenizer.cls_token_id or input_ids[idx] == tokenizer.sep_token_id or input_ids[idx] == tokenizer.pad_token_id:\n",
    "                sid = None\n",
    "            else:\n",
    "                sep_indices = [j for j, token_id in enumerate(input_ids) if token_id == tokenizer.sep_token_id]\n",
    "                if sep_indices:\n",
    "                    if pad_on_right:\n",
    "                        sid = 0 if idx < sep_indices[0] else 1\n",
    "                    else:\n",
    "                        sid = 1 if idx < sep_indices[0] else 0\n",
    "            sequence_ids.append(sid)\n",
    "        \n",
    "        pred_answer = extract_answer_from_predictions(\n",
    "            input_ids, pred_labels[i], offset_mapping, sequence_ids, \n",
    "            context_id=1 if pad_on_right else 0\n",
    "        )\n",
    "        \n",
    "        if hasattr(eval_dataset, 'features') and 'overflow_to_sample_mapping' in eval_dataset.features:\n",
    "            original_idx = eval_dataset[i].get('overflow_to_sample_mapping', i)\n",
    "        else:\n",
    "            original_idx = i % len(original_dataset)\n",
    "        \n",
    "        sample_to_predictions[original_idx].append(pred_answer)\n",
    "    \n",
    "    for sample_idx, pred_answers in sample_to_predictions.items():\n",
    "        if sample_idx < len(original_dataset):\n",
    "            true_answer = original_dataset[sample_idx][\"answer\"].strip().lower()\n",
    "            \n",
    "            best_f1 = 0.0\n",
    "            for pred_answer in pred_answers:\n",
    "                pred_answer = pred_answer.strip().lower()\n",
    "                if pred_answer == \"\":\n",
    "                    f1_scores.append(0.0)\n",
    "                    continue\n",
    "                true_chars = set(range(len(true_answer)))\n",
    "                pred_chars = set(range(len(pred_answer)))\n",
    "                common = min(len(true_chars), len(pred_chars))\n",
    "                precision = common / len(pred_chars) if len(pred_chars) > 0 else 0.0\n",
    "                recall = common / len(true_chars) if len(true_chars) > 0 else 0.0\n",
    "                f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "            f1_scores.append(best_f1)\n",
    "    \n",
    "    pred_labels_flat = pred_labels.flatten()\n",
    "    valid_preds = pred_labels_flat[pred_labels_flat != -100]\n",
    "    unique, counts = np.unique(valid_preds, return_counts=True)\n",
    "    pred_dist = dict(zip(unique, counts))\n",
    "    print(f\"Prediction distribution: {pred_dist}\")\n",
    "    \n",
    "    avg_f1 = np.mean(f1_scores) if f1_scores else 0.0\n",
    "    return avg_f1, len(f1_scores)\n",
    "\n",
    "def focal_loss(logits, labels, alpha=None, gamma=2.0, ignore_index=-100, label_smoothing=0.1):\n",
    "    ce_loss = F.cross_entropy(\n",
    "        logits, \n",
    "        labels, \n",
    "        weight=alpha, \n",
    "        reduction='none', \n",
    "        ignore_index=ignore_index,\n",
    "        label_smoothing=label_smoothing\n",
    "    )\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    focal_loss = ((1 - pt) ** gamma) * ce_loss\n",
    "    return focal_loss.mean()\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, focal_gamma=2.0, label_smoothing=0.1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        alpha = self.class_weights.to(model.device) if self.class_weights is not None else None\n",
    "        loss = focal_loss(\n",
    "            logits.view(-1, self.model.config.num_labels), \n",
    "            labels.view(-1),\n",
    "            alpha=alpha,\n",
    "            gamma=self.focal_gamma,\n",
    "            ignore_index=-100,\n",
    "            label_smoothing=self.label_smoothing\n",
    "        )\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training multilingual sequence labeler\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "all_labels = []\n",
    "for example in train_dataset:\n",
    "    if example[\"answer_start\"] == -1:\n",
    "        all_labels.append(0)\n",
    "    else:\n",
    "        all_labels.append(1)\n",
    "\n",
    "class_weights = compute_class_weights(all_labels)\n",
    "\n",
    "columns_to_remove = [col for col in train_dataset.column_names if col != \"lang\"]\n",
    "tokenized_train = train_dataset.map(\n",
    "    create_token_labels, \n",
    "    batched=True, \n",
    "    remove_columns=columns_to_remove\n",
    ")\n",
    "tokenized_val = val_dataset.map(\n",
    "    create_token_labels, \n",
    "    batched=True, \n",
    "    remove_columns=columns_to_remove\n",
    ")\n",
    "\n",
    "model_tc = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=len(label_list), \n",
    "    id2label=id_to_label, \n",
    "    label2id=label_to_id,\n",
    "    dropout=0.2,\n",
    "    attention_dropout=0.2,\n",
    ")\n",
    "\n",
    "args_tc = TrainingArguments(\n",
    "    output_dir=\"seq-lab-multilingual\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    warmup_ratio=0.1,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=[],\n",
    "    push_to_hub=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    adam_epsilon=1e-8,\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.999,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer_tc = WeightedTrainer(\n",
    "    model=model_tc,\n",
    "    args=args_tc,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_token_metrics,\n",
    "    class_weights=class_weights,\n",
    "    focal_gamma=2.5,\n",
    "    label_smoothing=0.1,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining...\")\n",
    "trainer_tc.train()\n",
    "\n",
    "print(f\"\\nEvaluating per language...\")\n",
    "results_by_lang = {}\n",
    "\n",
    "for lang in languages:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating on {lang.lower()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    lang_val = val_dataset.filter(lambda ex: ex[\"lang\"] == lang)\n",
    "    tokenized_lang_val = tokenized_val.filter(lambda ex: ex[\"lang\"] == lang)\n",
    "    \n",
    "    print(f\"\\nEvaluating token-level metrics...\")\n",
    "    token_metrics = trainer_tc.evaluate(tokenized_lang_val)\n",
    "    \n",
    "    print(f\"Computing F1...\")\n",
    "    f1_score, f1_count = compute_f1(trainer_tc, tokenized_lang_val, lang_val)\n",
    "    \n",
    "    results_by_lang[lang] = {\n",
    "        **token_metrics,\n",
    "        \"f1\": f1_score,\n",
    "        \"f1_count\": f1_count\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {lang.lower()}:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Token-level Accuracy:  {token_metrics['eval_accuracy']:.4f}\")\n",
    "    print(f\"F1 Score:              {f1_score:.4f} (n={f1_count})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CROSS-LANGUAGE COMPARISON\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Language':<12} {'Acc':<8}  {'F1':<8}\")\n",
    "print(f\"{'-'*60}\")\n",
    "lang_names = {\"ar\": \"Arabic\", \"ko\": \"Korean\", \"te\": \"Telugu\"}\n",
    "for lang in languages:\n",
    "    metrics = results_by_lang[lang]\n",
    "    print(f\"{lang_names.get(lang, lang).upper():<12} \"\n",
    "          f\"{metrics['eval_accuracy']:.4f}   \"\n",
    "          f\"{metrics['f1']:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "results_by_lang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
