{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60317ac1",
   "metadata": {},
   "source": [
    "# Word2Vec Bi-LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134cd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import regex as re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#Lemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt_tab')      \n",
    "nltk.download('wordnet')    \n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb4b74c",
   "metadata": {},
   "source": [
    "### Question Translation\n",
    "Word2Vec works on english words only, so it is essential to translate the questions from Korean, Arabic, and Telugu to English before leveraging it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953db192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Original Dataset\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "languages = [\"ar\", \"ko\", \"te\", \"en\"]\n",
    "train = dataset[\"train\"].filter(lambda example: example['lang'] in languages).to_pandas()\n",
    "val = dataset[\"validation\"].filter(lambda example: example['lang'] in languages).to_pandas()\n",
    "\n",
    "#GPU for acceleration if possible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "#Downloading sequence_train and Tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "\n",
    "def Translate(words, lang):\n",
    "    langs = {\"ar\": \"ara_Arab\", \"ko\": \"kor_Hang\", \"te\": \"tel_Telu\"}\n",
    "\n",
    "    tokenizer.src_lang = langs[lang]\n",
    "    inputs = tokenizer(words, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, max_length=512)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def TranslateDF(df):\n",
    "    df_translated = df.copy()\n",
    "    translated_questions = []\n",
    "\n",
    "    for i, row in tqdm(df.iterrows()):\n",
    "        lang = row[\"lang\"]\n",
    "        question = row[\"question\"]\n",
    "        translated = Translate([question], lang)\n",
    "        translated_questions.append(translated[0] if translated and translated[0] else question)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Translated {i+1}/{len(df)} questions...\")\n",
    "\n",
    "    df_translated[\"question\"] = translated_questions\n",
    "    return df_translated\n",
    "\n",
    "#Translate and save dataset\n",
    "if False:\n",
    "    val = TranslateDF(val)\n",
    "    val.to_parquet(\"validationEN.parquet\", index=False)\n",
    "    train = TranslateDF(train)\n",
    "    train.to_parquet(\"trainingEN.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aec60d",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load translated dataset and word2vec\n",
    "train = pd.read_parquet(\"trainingEN.parquet\")\n",
    "val = pd.read_parquet(\"validationEN.parquet\")\n",
    "\n",
    "w2v = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a807bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def Tokenize(sentence):\n",
    "    return np.array(re.findall(r\"\\w+\", sentence.lower()))\n",
    "\n",
    "def word2vec(word):\n",
    "  if word in w2v.key_to_index:\n",
    "      return w2v[word]\n",
    "\n",
    "#Embeds each lemmatized word in a sentence and calculates the mean, deeming it the sentence embedding \n",
    "def sentence2vec(sentence):\n",
    "    words = Tokenize(sentence)\n",
    "    sentence_vec = [word2vec(lemmatizer.lemmatize(word)) for word in words]\n",
    "    sentence_vec = [vec for vec in sentence_vec if vec is not None]\n",
    "\n",
    "    if len(sentence_vec) == 0:\n",
    "        return np.zeros(w2v.vector_size)\n",
    "\n",
    "    return np.mean(sentence_vec, axis=0)\n",
    "\n",
    "s = sentence2vec(\"This is a test sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "      def __init__(self, embedding_dim=300, hidden_dim=128, num_layers=1, num_classes=2):\n",
    "          super().__init__()\n",
    "          self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "          self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "          self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "      def forward(self, x):\n",
    "          output, (h_n, c_n) = self.lstm(x)\n",
    "          h_final = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim=1)\n",
    "          h_final = self.dropout(h_final)\n",
    "          return self.fc(h_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f0672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TrainAndValidate(train, val):\n",
    "\n",
    "    #Embedding\n",
    "    question_vecs_train = torch.tensor([sentence2vec(q) for q in train['question']])\n",
    "    context_vecs_train = torch.tensor([sentence2vec(c) for c in train['context']])\n",
    "\n",
    "    question_vecs_val = torch.tensor([sentence2vec(q) for q in val['question']])\n",
    "    context_vecs_val = torch.tensor([sentence2vec(c) for c in val['context']])\n",
    "\n",
    "    labels_train = torch.tensor([1 if answerable else 0 for answerable in train[\"answerable\"]])\n",
    "    labels_val = torch.tensor([1 if answerable else 0 for answerable in val[\"answerable\"]])\n",
    "    \n",
    "    sequence_train = torch.stack([context_vecs_train, question_vecs_train], dim=0)\n",
    "    sequence_val = torch.stack([context_vecs_val, question_vecs_val], dim=0)\n",
    "\n",
    "    #Data Organization\n",
    "    X_train = sequence_train.permute(1, 0, 2) \n",
    "    X_test = sequence_val.permute(1, 0, 2)\n",
    "\n",
    "    train_ds = TensorDataset(X_train, labels_train)\n",
    "    test_ds = TensorDataset(X_test, labels_val)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    test_dl = DataLoader(test_ds, batch_size=64)\n",
    "\n",
    "    #Model Creation\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = BiLSTMClassifier().to(device)\n",
    "    model = model.double()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    #Training\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in tqdm(train_dl):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb.to(torch.float64))\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dl):.4f}\")\n",
    "\n",
    "    #Evaluate\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_dl:\n",
    "            out = model(xb.to(torch.float64))\n",
    "            preds = out.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    print(f\"Test Accuracy: {correct / total:.2%}\")\n",
    "\n",
    "\n",
    "train_ar = train[train['lang'] == 'ar']\n",
    "train_ko = train[train['lang'] == 'ko']\n",
    "train_te = train[train['lang'] == 'te']\n",
    "\n",
    "val_ar = val[val['lang'] == 'ar']\n",
    "val_ko = val[val['lang'] == 'ko']\n",
    "val_te = val[val['lang'] == 'te']\n",
    "\n",
    "TrainAndValidate(train_ar, val_ar)\n",
    "TrainAndValidate(train_ko, val_ko)\n",
    "TrainAndValidate(train_te, val_te)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
