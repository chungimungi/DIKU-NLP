{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tnf04DBFw_I5",
        "outputId": "f6b1fda8-2ffc-4dea-9cc7-70ea738945f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample from train dataset:\n",
            "Keys: dict_keys(['question', 'context', 'lang', 'answerable', 'answer_start', 'answer', 'answer_inlang'])\n",
            "Answer structure: France\n",
            "Answer type: <class 'str'>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenizing datasets...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6335 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1155 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\aarus\\AppData\\Local\\Temp\\ipykernel_10668\\3981929279.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "wandb: Currently logged in as: aarushsinha60 (chungimungi) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
            "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\aarus\\Desktop\\Github\\DIKU-NLP\\week-37\\wandb\\run-20251022_182627-zehs6tv5</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/chungimungi/huggingface/runs/zehs6tv5' target=\"_blank\">vague-dew-29</a></strong> to <a href='https://wandb.ai/chungimungi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/chungimungi/huggingface' target=\"_blank\">https://wandb.ai/chungimungi/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/chungimungi/huggingface/runs/zehs6tv5' target=\"_blank\">https://wandb.ai/chungimungi/huggingface/runs/zehs6tv5</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1224' max='1224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1224/1224 19:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.929300</td>\n",
              "      <td>2.812973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.613700</td>\n",
              "      <td>2.686838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.478600</td>\n",
              "      <td>2.355334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.236800</td>\n",
              "      <td>2.183851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.041000</td>\n",
              "      <td>1.975945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.959900</td>\n",
              "      <td>1.698209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.816600</td>\n",
              "      <td>1.666332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.658300</td>\n",
              "      <td>1.659788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.406400</td>\n",
              "      <td>1.585595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.360700</td>\n",
              "      <td>1.591094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.428400</td>\n",
              "      <td>1.535704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.357100</td>\n",
              "      <td>1.547023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.405900</td>\n",
              "      <td>1.482523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.328300</td>\n",
              "      <td>1.481432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.340400</td>\n",
              "      <td>1.477129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.278100</td>\n",
              "      <td>1.454158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.092000</td>\n",
              "      <td>1.512422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.107700</td>\n",
              "      <td>1.480958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.999000</td>\n",
              "      <td>1.490729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.978700</td>\n",
              "      <td>1.500679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.019200</td>\n",
              "      <td>1.463367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.989600</td>\n",
              "      <td>1.487551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.050600</td>\n",
              "      <td>1.447159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.002100</td>\n",
              "      <td>1.445925</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Overall Evaluation\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [75/75 00:15]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Perplexity: 4.25\n",
            "Overall Loss: 1.4472\n",
            "\n",
            "Language-specific Evaluations\n",
            "\n",
            "Evaluating AR\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/1155 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of AR validation examples: 415\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/415 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\aarus\\AppData\\Local\\Temp\\ipykernel_10668\\3981929279.py:128: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  lang_trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [28/28 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AR Perplexity: 3.92\n",
            "AR Loss: 1.3671\n",
            "\n",
            "Evaluating KO\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/1155 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of KO validation examples: 356\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/356 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23/23 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KO Perplexity: 3.62\n",
            "KO Loss: 1.2862\n",
            "\n",
            "Evaluating TE\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/1155 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of TE validation examples: 384\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/384 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TE Perplexity: 5.41\n",
            "TE Loss: 1.6876\n",
            "\n",
            "Evaluating English Contexts Only\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1155 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\aarus\\AppData\\Local\\Temp\\ipykernel_10668\\3981929279.py:156: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  en_trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [75/75 00:15]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English Context Perplexity: 9.67\n",
            "English Context Loss: 2.2691\n",
            "English Context eval_model_preparation_time: 0.0018\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "import torch\n",
        "import math\n",
        "\n",
        "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
        "\n",
        "languages = ['ar', 'ko', 'te']\n",
        "train_dataset = dataset[\"train\"].filter(lambda example: example['lang'] in languages)\n",
        "val_dataset = dataset[\"validation\"].filter(lambda example: example['lang'] in languages)\n",
        "\n",
        "print(\"Sample from train dataset:\")\n",
        "sample = train_dataset[0]\n",
        "print(f\"Keys: {sample.keys()}\")\n",
        "print(f\"Answer structure: {sample['answer']}\")\n",
        "print(f\"Answer type: {type(sample['answer'])}\")\n",
        "\n",
        "#model loading\n",
        "model_checkpoint = \"google-bert/bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "max_length = 384\n",
        "doc_stride = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    contexts = [c.strip() for c in examples[\"context\"]]\n",
        "\n",
        "    tokenized_examples = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        truncation=\"only_second\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        sample_index = sample_mapping[i]\n",
        "\n",
        "        start_char = examples[\"answer_start\"][sample_index]\n",
        "        answer_text = examples[\"answer\"][sample_index]\n",
        "        end_char = start_char + len(answer_text)\n",
        "\n",
        "        token_start_index = 0\n",
        "        while token_start_index < len(sequence_ids) and sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while token_end_index >= 0 and sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "            while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "print(\"\\nTokenizing datasets...\")\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nOverall Evaluation\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Overall Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
        "print(f\"Overall Loss: {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "print(\"\\nLanguage-specific Evaluations\")\n",
        "\n",
        "for lang in languages:\n",
        "    print(f\"\\nEvaluating {lang.upper()}\")\n",
        "    lang_val_dataset = val_dataset.filter(lambda example: example['lang'] == lang)\n",
        "    print(f\"Number of {lang.upper()} validation examples: {len(lang_val_dataset)}\")\n",
        "\n",
        "    if len(lang_val_dataset) == 0:\n",
        "        print(f\"No validation examples found for language: {lang}\")\n",
        "        continue\n",
        "\n",
        "    tokenized_lang_val = lang_val_dataset.map(\n",
        "        preprocess_function,\n",
        "        batched=True,\n",
        "        remove_columns=lang_val_dataset.column_names\n",
        "    )\n",
        "\n",
        "    lang_trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        eval_dataset=tokenized_lang_val,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    lang_eval_results = lang_trainer.evaluate()\n",
        "    print(f\"{lang.upper()} Perplexity: {math.exp(lang_eval_results['eval_loss']):.2f}\")\n",
        "    print(f\"{lang.upper()} Loss: {lang_eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "# English Context Only Evaluation\n",
        "print(\"\\nEvaluating English Contexts Only\")\n",
        "\n",
        "en_contexts = list(val_dataset[\"context\"])\n",
        "en_context_only_dataset = Dataset.from_dict({\n",
        "    \"question\": [\"\"] * len(en_contexts),  # empty question (no lang)\n",
        "    \"context\": en_contexts,\n",
        "    \"answer\": [\"\"] * len(en_contexts),\n",
        "    \"answer_start\": [0] * len(en_contexts),\n",
        "})\n",
        "\n",
        "tokenized_en_context_val = en_context_only_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=en_context_only_dataset.column_names\n",
        ")\n",
        "\n",
        "en_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    eval_dataset=tokenized_en_context_val,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "en_eval_results = en_trainer.evaluate()\n",
        "print(f\"English Context Perplexity: {math.exp(en_eval_results['eval_loss']):.2f}\")\n",
        "print(f\"English Context Loss: {en_eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "for key, value in en_eval_results.items():\n",
        "    if key not in ['eval_loss', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']:\n",
        "        print(f\"English Context {key}: {value:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}