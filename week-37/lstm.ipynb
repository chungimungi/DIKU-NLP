{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed70134",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "### Fetching and Organizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98632908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aarus/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\aarus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\aarus/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\aarus/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "#Lemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt_tab')      \n",
    "nltk.download('wordnet')    \n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "\n",
    "languages = ['ar', 'ko', 'te']\n",
    "train_df = dataset[\"train\"].filter(lambda example: example['lang'] in languages).to_pandas()\n",
    "val_df = dataset[\"validation\"].filter(lambda example: example['lang'] in languages).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4c425",
   "metadata": {},
   "source": [
    "### GloVe Embeddings and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f18ae43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings (glove-wiki-gigaword-300)...\n",
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
      "Loaded 400000 word vectors with dimensionality 300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Loading GloVe embeddings (glove-wiki-gigaword-300)...\")\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")\n",
    "print(f\"Loaded {len(glove)} word vectors with dimensionality {glove.vector_size}\")\n",
    "\n",
    "w2v = glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8293d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def Tokenize(sentence):\n",
    "    return np.array(re.findall(r\"\\w+\", sentence.lower()))\n",
    "\n",
    "def word2vec(word):\n",
    "  if word in w2v.key_to_index:\n",
    "      return w2v[word]\n",
    "\n",
    "#Embeds each lemmatized word in a sentence and calculates the mean, deeming it the sentence embedding \n",
    "def sentence2vec(sentence):\n",
    "    words = Tokenize(sentence)\n",
    "    sentence_vec = [word2vec(lemmatizer.lemmatize(word)) for word in words]\n",
    "    sentence_vec = [vec for vec in sentence_vec if vec is not None]\n",
    "\n",
    "    if len(sentence_vec) == 0:\n",
    "        return np.zeros(w2v.vector_size)\n",
    "\n",
    "    return np.mean(sentence_vec, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc38a52e",
   "metadata": {},
   "source": [
    "### DataSet Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff2be8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Arabic Training with 2558 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Arabic Validation with 415 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Korean Training with 2422 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Korean Validation with 356 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Telugu Training with 1355 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Telugu Validation with 384 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded English Training with 6335 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded English Validation with 1155 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, sentences : np.ndarray, name):\n",
    "        self.name = name\n",
    "        self.sentences = torch.FloatTensor(sentences)\n",
    "        print(f\"Loaded {name} with {len(self.sentences)} sentences\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.sentences[idx]\n",
    "    \n",
    "\n",
    "def CreateDataLoader(sentences, name):\n",
    "    embedings = np.array([sentence2vec(s) for s in tqdm(sentences, desc=f\"Embedding {name}\", leave=False)])\n",
    "    dataset = SentenceDataset(embedings, name)\n",
    "    # Use CUDA generator to match model device\n",
    "    generator = torch.Generator(device='cuda')\n",
    "    return DataLoader(dataset, batch_size=32, shuffle=True, generator=generator)\n",
    "\n",
    "\n",
    "def CreateDataLoaders(train_df, val_df, field, lang, name):\n",
    "    train_dl = CreateDataLoader(train_df[train_df['lang'].isin(lang)][field], f\"{name} Training\")\n",
    "    val_dl = CreateDataLoader(val_df[val_df['lang'].isin(lang)][field],  f\"{name} Validation\")\n",
    "    return train_dl, val_dl\n",
    "\n",
    "ar_train, ar_val = CreateDataLoaders(train_df, val_df, \"question\", [\"ar\"], \"Arabic\")\n",
    "ko_train, ko_val = CreateDataLoaders(train_df, val_df, \"question\", [\"ko\"], \"Korean\")\n",
    "te_train, te_val = CreateDataLoaders(train_df, val_df, \"question\", [\"te\"], \"Telugu\")\n",
    "en_train, en_val = CreateDataLoaders(train_df, val_df, \"context\", [\"ar\", \"ko\", \"te\"], \"English\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f11c98",
   "metadata": {},
   "source": [
    "### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b1a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEmbeddingModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_layers=2, dropout=0.3):\n",
    "        super(LSTMEmbeddingModel, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = x.unsqueeze(1)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        output = self.fc(lstm_out)\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
    "    \n",
    "    def trainSelf(self, dataloader, epochs=10, learning_rate=0.001, display=False, feedback=False):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(device)\n",
    "        \n",
    "        criterion = nn.CosineEmbeddingLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        self.train()\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for _, (data, targets) in enumerate(dataloader):\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output, _ = self(data)\n",
    "                \n",
    "                # CosineEmbeddingLoss expects a target of 1 or -1 for similarity/dissimilarity\n",
    "                target_labels = torch.ones(data.size(0), device=device)  # make embeddings similar\n",
    "                loss = criterion(output, targets, target_labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=5)\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            losses.append(avg_loss)\n",
    "            if feedback:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    def perplexity(self, dataloader):\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        criterion = nn.CosineEmbeddingLoss(reduction='sum')\n",
    "        device = next(self.parameters()).device  # Get device from model parameters\n",
    "        \n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, targets in dataloader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                output, _ = self(data)\n",
    "                target_labels = torch.ones(data.size(0), device=device)\n",
    "                loss = criterion(output, targets, target_labels)\n",
    "                total_loss += loss.item()\n",
    "                total_samples += data.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else float('inf')\n",
    "        perplexity = math.exp(avg_loss)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357159f2",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8d516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TrainAndEvaluateModel(train_dl, val_dl, title):\n",
    "\n",
    "    model = LSTMEmbeddingModel(input_dim=300, hidden_dim=256, num_layers=2, dropout=0.3)\n",
    "\n",
    "    #Training\n",
    "    model.trainSelf(train_dl, epochs=20, learning_rate=0.001)\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    #Validation\n",
    "    perplexity = model.perplexity(val_dl)\n",
    "\n",
    "    print(f\"============ { title } ============\")\n",
    "    print(f\"Perplexity: {perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bae9990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Arabic ============\n",
      "Perplexity: 1.7146\n",
      "============ Korean ============\n",
      "Perplexity: 2.6645\n",
      "============ Telugu ============\n",
      "Perplexity: 2.4619\n",
      "============ English ============\n",
      "Perplexity: 1.0109\n"
     ]
    }
   ],
   "source": [
    "TrainAndEvaluateModel(ar_train, ar_val, title=\"Arabic\")\n",
    "TrainAndEvaluateModel(ko_train, ko_val, title=\"Korean\")\n",
    "TrainAndEvaluateModel(te_train, te_val, title=\"Telugu\")\n",
    "TrainAndEvaluateModel(en_train, en_val, title=\"English\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
