{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7be826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training token-classifier for language: ar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aarus\\AppData\\Local\\Temp\\ipykernel_31388\\3895613878.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_tc = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='334' max='334' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [334/334 01:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.101309</td>\n",
       "      <td>0.598485</td>\n",
       "      <td>0.045559</td>\n",
       "      <td>0.084673</td>\n",
       "      <td>0.972713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ar: {'eval_loss': 0.10130859166383743, 'eval_precision': 0.5984848484848485, 'eval_recall': 0.04555940023068051, 'eval_f1': 0.08467309753483387, 'eval_accuracy': 0.972712603645775, 'eval_runtime': 2.9675, 'eval_samples_per_second': 146.926, 'eval_steps_per_second': 18.534, 'epoch': 1.0}\n",
      "Training token-classifier for language: ko\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b0ee9c9caa4c4e9544c4b5f845b249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69bd141b2494f108d6011b4c897bfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0399e53bd0b44528ef1785023356fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2422 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0d2272de97461f8c6d838ec63ce499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aarus\\AppData\\Local\\Temp\\ipykernel_31388\\3895613878.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_tc = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 01:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0.113132</td>\n",
       "      <td>0.630662</td>\n",
       "      <td>0.111453</td>\n",
       "      <td>0.189430</td>\n",
       "      <td>0.967272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ko: {'eval_loss': 0.11313216388225555, 'eval_precision': 0.6306620209059234, 'eval_recall': 0.11145320197044335, 'eval_f1': 0.18942961800104657, 'eval_accuracy': 0.9672723431227551, 'eval_runtime': 2.3927, 'eval_samples_per_second': 151.295, 'eval_steps_per_second': 19.225, 'epoch': 1.0}\n",
      "Training token-classifier for language: te\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b6c794dd8b400eb51eff9fb6f42fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f33888bbe73410dbe3ada5c45abe34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b604a1909d4abea59e517de30a80ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109dfab743394edf87c9b25d7e824f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aarus\\AppData\\Local\\Temp\\ipykernel_31388\\3895613878.py:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_tc = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='172' max='172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [172/172 00:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.118100</td>\n",
       "      <td>0.078299</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.015831</td>\n",
       "      <td>0.030717</td>\n",
       "      <td>0.980213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49/49 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for te: {'eval_loss': 0.07829934358596802, 'eval_precision': 0.5142857142857142, 'eval_recall': 0.0158311345646438, 'eval_f1': 0.030716723549488054, 'eval_accuracy': 0.9802125065319631, 'eval_runtime': 2.5508, 'eval_samples_per_second': 152.108, 'eval_steps_per_second': 19.209, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ar': {'eval_loss': 0.10130859166383743,\n",
       "  'eval_precision': 0.5984848484848485,\n",
       "  'eval_recall': 0.04555940023068051,\n",
       "  'eval_f1': 0.08467309753483387,\n",
       "  'eval_accuracy': 0.972712603645775,\n",
       "  'eval_runtime': 2.9675,\n",
       "  'eval_samples_per_second': 146.926,\n",
       "  'eval_steps_per_second': 18.534,\n",
       "  'epoch': 1.0},\n",
       " 'ko': {'eval_loss': 0.11313216388225555,\n",
       "  'eval_precision': 0.6306620209059234,\n",
       "  'eval_recall': 0.11145320197044335,\n",
       "  'eval_f1': 0.18942961800104657,\n",
       "  'eval_accuracy': 0.9672723431227551,\n",
       "  'eval_runtime': 2.3927,\n",
       "  'eval_samples_per_second': 151.295,\n",
       "  'eval_steps_per_second': 19.225,\n",
       "  'epoch': 1.0},\n",
       " 'te': {'eval_loss': 0.07829934358596802,\n",
       "  'eval_precision': 0.5142857142857142,\n",
       "  'eval_recall': 0.0158311345646438,\n",
       "  'eval_f1': 0.030716723549488054,\n",
       "  'eval_accuracy': 0.9802125065319631,\n",
       "  'eval_runtime': 2.5508,\n",
       "  'eval_samples_per_second': 152.108,\n",
       "  'eval_steps_per_second': 19.209,\n",
       "  'epoch': 1.0}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "label_list = [\"O\", \"ANS\"]\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "max_length = 384\n",
    "doc_stride = 128\n",
    "\n",
    "\n",
    "def create_token_labels(examples):\n",
    "    # Build token-level labels for the context part only; question tokens get label -100\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offsets_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    labels = []\n",
    "    for i, offsets in enumerate(offsets_mapping):\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_idx = sample_mapping[i]\n",
    "        answer_start = examples[\"answer_start\"][sample_idx]\n",
    "        answer_text = examples[\"answer\"][sample_idx]\n",
    "        answer_end = -1 if answer_start == -1 else answer_start + len(answer_text)\n",
    "\n",
    "        example_labels = []\n",
    "        context_id = 1 if pad_on_right else 0\n",
    "        for idx, offset in enumerate(offsets):\n",
    "            if sequence_ids[idx] is None:\n",
    "                example_labels.append(-100)\n",
    "            elif sequence_ids[idx] != context_id:\n",
    "                example_labels.append(-100)\n",
    "            else:\n",
    "                if answer_start == -1 or offset is None:\n",
    "                    example_labels.append(label_to_id[\"O\"])  # unanswerable â†’ no tokens\n",
    "                else:\n",
    "                    start, end = offset\n",
    "                    if start >= answer_end or end <= answer_start:\n",
    "                        example_labels.append(label_to_id[\"O\"])  # outside answer span\n",
    "                    else:\n",
    "                        example_labels.append(label_to_id[\"ANS\"])  # overlaps answer span\n",
    "        labels.append(example_labels)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def compute_token_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    # Only evaluate on context tokens (labels != -100)\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for p, l in zip(preds, labels):\n",
    "        for pi, li in zip(p, l):\n",
    "            if li != -100:\n",
    "                true_labels.append(li)\n",
    "                pred_labels.append(pi)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred_labels, labels=[label_to_id[\"ANS\"]], average=\"binary\")\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "\n",
    "results_by_lang = {}\n",
    "for lang in languages:\n",
    "    print(f\"Training token-classifier for language: {lang}\")\n",
    "    lang_train = train_dataset.filter(lambda ex: ex[\"lang\"] == lang)\n",
    "    lang_val = val_dataset.filter(lambda ex: ex[\"lang\"] == lang)\n",
    "\n",
    "    tokenized_lang_train = lang_train.map(create_token_labels, batched=True, remove_columns=lang_train.column_names)\n",
    "    tokenized_lang_val = lang_val.map(create_token_labels, batched=True, remove_columns=lang_val.column_names)\n",
    "\n",
    "    model_tc = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), id2label=id_to_label, label2id=label_to_id)\n",
    "    args_tc = TrainingArguments(\n",
    "        output_dir=f\"seq-lab-{lang}\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=[],\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    trainer_tc = Trainer(\n",
    "        model=model_tc,\n",
    "        args=args_tc,\n",
    "        train_dataset=tokenized_lang_train,\n",
    "        eval_dataset=tokenized_lang_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_token_metrics,\n",
    "    )\n",
    "\n",
    "    trainer_tc.train()\n",
    "    metrics = trainer_tc.evaluate()\n",
    "    results_by_lang[lang] = metrics\n",
    "    print(f\"Results for {lang}: {metrics}\")\n",
    "\n",
    "results_by_lang\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
