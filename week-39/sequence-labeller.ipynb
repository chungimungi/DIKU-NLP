{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7be826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "full_dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "\n",
    "model_checkpoint = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "label_list = [\"O\", \"ANS\"]\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "max_length = 384\n",
    "doc_stride = 128\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "languages = [\"ar\", \"ko\", \"te\"]\n",
    "\n",
    "train_dataset = full_dataset[\"train\"]\n",
    "val_dataset = full_dataset[\"validation\"]\n",
    "\n",
    "def analyze_class_distribution(dataset, lang):\n",
    "    labels = []\n",
    "    for example in dataset:\n",
    "        if example[\"answer_start\"] == -1:\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    \n",
    "    counter = Counter(labels)\n",
    "    total = len(labels)\n",
    "    \n",
    "    print(f\"\\n--- Class Distribution for {lang.lower()} ---\")\n",
    "    print(f\"Class 0 (Not Answerable): {counter[0]} ({counter[0]/total:.2%})\")\n",
    "    print(f\"Class 1 (Answerable): {counter[1]} ({counter[1]/total:.2%})\")\n",
    "    print(f\"Imbalance Ratio: {max(counter.values()) / min(counter.values()):.2f}:1\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    class_weights = compute_class_weight('balanced', classes=unique_labels, y=labels)\n",
    "    class_weights[1] *= 100\n",
    "    class_weight_dict = dict(zip(unique_labels, class_weights))\n",
    "    \n",
    "    print(f\"Computed class weights: {class_weight_dict}\")\n",
    "    return torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "def create_token_labels(examples):\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offsets_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    labels = []\n",
    "    langs = []  # Preserve language information\n",
    "    for i, offsets in enumerate(offsets_mapping):\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        sample_idx = sample_mapping[i]\n",
    "        answer_start = examples[\"answer_start\"][sample_idx]\n",
    "        answer_text = examples[\"answer\"][sample_idx]\n",
    "        answer_end = -1 if answer_start == -1 else answer_start + len(answer_text)\n",
    "        \n",
    "        # Preserve the language for this tokenized sample\n",
    "        langs.append(examples[\"lang\"][sample_idx])\n",
    "\n",
    "        example_labels = []\n",
    "        context_id = 1 if pad_on_right else 0\n",
    "        \n",
    "        for idx, offset in enumerate(offsets):\n",
    "            if sequence_ids[idx] is None:\n",
    "                example_labels.append(-100)\n",
    "            elif sequence_ids[idx] != context_id:\n",
    "                example_labels.append(-100)\n",
    "            else:\n",
    "                if answer_start == -1 or offset is None:\n",
    "                    example_labels.append(label_to_id[\"O\"])\n",
    "                else:\n",
    "                    start, end = offset\n",
    "                    if start >= answer_end or end <= answer_start:\n",
    "                        example_labels.append(label_to_id[\"O\"])\n",
    "                    else:\n",
    "                        example_labels.append(label_to_id[\"ANS\"])\n",
    "        labels.append(example_labels)\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    tokenized[\"lang\"] = langs  # Add language column\n",
    "    tokenized[\"offset_mapping\"] = offsets_mapping\n",
    "    return tokenized\n",
    "\n",
    "def extract_answer_from_predictions(input_ids, predictions, offset_mapping, sequence_ids, context_id=1):\n",
    "    ans_indices = [i for i, (pred, seq_id) in enumerate(zip(predictions, sequence_ids)) \n",
    "                   if pred == label_to_id[\"ANS\"] and seq_id == context_id]\n",
    "    \n",
    "    if not ans_indices:\n",
    "        return \"\"\n",
    "    \n",
    "    start_idx = ans_indices[0]\n",
    "    end_idx = ans_indices[0]\n",
    "    \n",
    "    for i in range(1, len(ans_indices)):\n",
    "        if ans_indices[i] == ans_indices[i-1] + 1:\n",
    "            end_idx = ans_indices[i]\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if offset_mapping[start_idx] is not None and offset_mapping[end_idx] is not None:\n",
    "        char_start = offset_mapping[start_idx][0]\n",
    "        char_end = offset_mapping[end_idx][1]\n",
    "        \n",
    "        tokens = input_ids[start_idx:end_idx+1]\n",
    "        pred_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        return pred_text.strip()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def compute_token_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    for p, l in zip(preds, labels):\n",
    "        for pi, li in zip(p, l):\n",
    "            if li != -100:\n",
    "                true_labels.append(li)\n",
    "                pred_labels.append(pi)\n",
    "    \n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc\n",
    "    }\n",
    "\n",
    "def compute_exact_match(trainer, eval_dataset, original_dataset):\n",
    "    predictions = trainer.predict(eval_dataset)\n",
    "    pred_logits = predictions.predictions\n",
    "    pred_labels = np.argmax(pred_logits, axis=-1)\n",
    "    \n",
    "    exact_matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    sample_to_predictions = {}\n",
    "    \n",
    "    for i in range(len(eval_dataset)):\n",
    "        input_ids = eval_dataset[i][\"input_ids\"]\n",
    "        offset_mapping = eval_dataset[i].get(\"offset_mapping\", None)\n",
    "        \n",
    "        sequence_ids = []\n",
    "        for idx in range(len(input_ids)):\n",
    "            sid = None\n",
    "            if input_ids[idx] == tokenizer.cls_token_id or input_ids[idx] == tokenizer.sep_token_id or input_ids[idx] == tokenizer.pad_token_id:\n",
    "                sid = None\n",
    "            else:\n",
    "                sep_indices = [j for j, token_id in enumerate(input_ids) if token_id == tokenizer.sep_token_id]\n",
    "                if sep_indices:\n",
    "                    if pad_on_right:\n",
    "                        sid = 0 if idx < sep_indices[0] else 1\n",
    "                    else:\n",
    "                        sid = 1 if idx < sep_indices[0] else 0\n",
    "            sequence_ids.append(sid)\n",
    "        \n",
    "        pred_answer = extract_answer_from_predictions(\n",
    "            input_ids, pred_labels[i], offset_mapping, sequence_ids, \n",
    "            context_id=1 if pad_on_right else 0\n",
    "        )\n",
    "        \n",
    "        if hasattr(eval_dataset, 'features') and 'overflow_to_sample_mapping' in eval_dataset.features:\n",
    "            original_idx = eval_dataset[i].get('overflow_to_sample_mapping', i)\n",
    "        else:\n",
    "            original_idx = i % len(original_dataset)\n",
    "        \n",
    "        if original_idx not in sample_to_predictions:\n",
    "            sample_to_predictions[original_idx] = []\n",
    "        sample_to_predictions[original_idx].append(pred_answer)\n",
    "    \n",
    "    for sample_idx, pred_answers in sample_to_predictions.items():\n",
    "        if sample_idx < len(original_dataset):\n",
    "            true_answer = original_dataset[sample_idx][\"answer\"]\n",
    "            pred_answer = next((p for p in pred_answers if p), \"\")\n",
    "            \n",
    "            true_norm = true_answer.strip().lower()\n",
    "            pred_norm = pred_answer.strip().lower()\n",
    "            \n",
    "            if true_norm == pred_norm:\n",
    "                exact_matches += 1\n",
    "            total += 1\n",
    "    \n",
    "    pred_labels_flat = pred_labels.flatten()\n",
    "    valid_preds = pred_labels_flat[pred_labels_flat != -100]\n",
    "    unique, counts = np.unique(valid_preds, return_counts=True)\n",
    "    pred_dist = dict(zip(unique, counts))\n",
    "    print(f\"Prediction distribution: {pred_dist}\")\n",
    "    \n",
    "    em_score = exact_matches / total if total > 0 else 0\n",
    "    return em_score, exact_matches, total\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        if self.class_weights is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(\n",
    "                weight=self.class_weights.to(model.device),\n",
    "                ignore_index=-100\n",
    "            )\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training multilingual sequence labeler\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "all_labels = []\n",
    "for example in train_dataset:\n",
    "    if example[\"answer_start\"] == -1:\n",
    "        all_labels.append(0)\n",
    "    else:\n",
    "        all_labels.append(1)\n",
    "\n",
    "class_weights = compute_class_weights(all_labels)\n",
    "\n",
    "columns_to_remove = [col for col in train_dataset.column_names if col != \"lang\"]\n",
    "tokenized_train = train_dataset.map(\n",
    "    create_token_labels, \n",
    "    batched=True, \n",
    "    remove_columns=columns_to_remove\n",
    ")\n",
    "tokenized_val = val_dataset.map(\n",
    "    create_token_labels, \n",
    "    batched=True, \n",
    "    remove_columns=columns_to_remove\n",
    ")\n",
    "\n",
    "model_tc = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    num_labels=len(label_list), \n",
    "    id2label=id_to_label, \n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "args_tc = TrainingArguments(\n",
    "    output_dir=\"seq-lab-multilingual\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=[],\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer_tc = WeightedTrainer(\n",
    "    model=model_tc,\n",
    "    args=args_tc,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_token_metrics,\n",
    "    class_weights=class_weights,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# Comment out training to load from checkpoint instead\n",
    "# print(f\"\\nTraining...\")\n",
    "# trainer_tc.train()\n",
    "\n",
    "# Load from checkpoint\n",
    "print(f\"\\nLoading from checkpoint...\")\n",
    "checkpoint_path = \"seq-lab-multilingual\"  \n",
    "model_tc = AutoModelForTokenClassification.from_pretrained(checkpoint_path)\n",
    "trainer_tc.model = model_tc\n",
    "\n",
    "print(f\"\\nEvaluating per language...\")\n",
    "results_by_lang = {}\n",
    "\n",
    "for lang in languages:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating on {lang.lower()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    lang_val = val_dataset.filter(lambda ex: ex[\"lang\"] == lang)\n",
    "    tokenized_lang_val = tokenized_val.filter(lambda ex: ex[\"lang\"] == lang)\n",
    "    \n",
    "    print(f\"\\nEvaluating token-level metrics...\")\n",
    "    token_metrics = trainer_tc.evaluate(tokenized_lang_val)\n",
    "    \n",
    "    print(f\"Computing exact match...\")\n",
    "    em_score, em_count, em_total = compute_exact_match(trainer_tc, tokenized_lang_val, lang_val)\n",
    "    \n",
    "    results_by_lang[lang] = {\n",
    "        **token_metrics,\n",
    "        \"exact_match\": em_score,\n",
    "        \"exact_match_count\": f\"{em_count}/{em_total}\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results for {lang.lower()}:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Token-level Accuracy:  {token_metrics['eval_accuracy']:.4f}\")\n",
    "    print(f\"Exact Match:           {em_score:.4f} ({em_count}/{em_total})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"{'Language':<12} {'Acc':<8}  {'EM':<8}\")\n",
    "print(f\"{'-'*60}\")\n",
    "for lang in languages:\n",
    "    metrics = results_by_lang[lang]\n",
    "    print(f\"{lang.lower():<12} \"\n",
    "          f\"{metrics['eval_accuracy']:.4f}\"\n",
    "          f\"{metrics['exact_match']:.4f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "results_by_lang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
