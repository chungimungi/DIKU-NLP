{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6120c207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/hyperturing1/0/aarushs/miniconda3/lib/python3.13/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telugu training examples: 1355\n",
      "Telugu validation examples: 384\n",
      "\n",
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c9eff58b074ef4a689664ef78440e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/384 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized lengths:\n",
      "  Input IDs: 223\n",
      "  Labels: 3\n",
      "  Non -100 labels: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Model 1: Telugu Question + English Context → Telugu Answer\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",

     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [116/116 07:02, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.790200</td>\n",
       "      <td>4.339398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.790200</td>\n",
       "      <td>3.372892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.790200</td>\n",
       "      <td>3.087343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.790200</td>\n",
       "      <td>2.918082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/hyperturing1/0/aarushs/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py:4037: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Model 2: Telugu Question → Telugu Answer\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/hyperturing1/0/aarushs/tmp/ipykernel_2924463/1918569618.py:215: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [116/116 03:03, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.639000</td>\n",
       "      <td>5.740492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.639000</td>\n",
       "      <td>4.923953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.639000</td>\n",
       "      <td>4.592388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.639000</td>\n",
       "      <td>4.510159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/hyperturing1/0/aarushs/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py:4037: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training Model 3: Telugu Question → English Answer\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/hyperturing1/0/aarushs/tmp/ipykernel_2924463/1918569618.py:215: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='116' max='116' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [116/116 03:04, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.657900</td>\n",
       "      <td>4.899090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.657900</td>\n",
       "      <td>4.416860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.657900</td>\n",
       "      <td>4.291759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.657900</td>\n",
       "      <td>4.227438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/hyperturing1/0/aarushs/miniconda3/lib/python3.13/site-packages/transformers/modeling_utils.py:4037: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "DETAILED RESULTS COMPARISON\n",
      "====================================================================================================\n",
      "\n",
      "==================== MODEL 1: Q+C → A_te ====================\n",
      "Training Loss: 3.2922\n",
      "Answerable Examples: 200\n",
      "Unanswerable Examples: 0\n",
      "\n",
      "Answerable Performance:\n",
      "  ROUGE-1: 0.5092\n",
      "  ROUGE-2: 0.2658\n",
      "  ROUGE-L: 0.5106\n",
      "  BLEU: 0.2541\n",
      "\n",
      "Unanswerable Performance:\n",
      "  ROUGE-1: 0\n",
      "  ROUGE-2: 0\n",
      "  ROUGE-L: 0\n",
      "  BLEU: 0\n",
      "\n",
      "Overall Performance:\n",
      "  ROUGE-1: 0.5092\n",
      "  ROUGE-2: 0.2658\n",
      "  ROUGE-L: 0.5106\n",
      "  BLEU: 0.2541\n",
      "\n",
      "==================== MODEL 2: Q → A_te ====================\n",
      "Training Loss: 5.2182\n",
      "Answerable Examples: 200\n",
      "Unanswerable Examples: 0\n",
      "\n",
      "Answerable Performance:\n",
      "  ROUGE-1: 0.0662\n",
      "  ROUGE-2: 0.0225\n",
      "  ROUGE-L: 0.0656\n",
      "  BLEU: 0.0\n",
      "\n",
      "Unanswerable Performance:\n",
      "  ROUGE-1: 0\n",
      "  ROUGE-2: 0\n",
      "  ROUGE-L: 0\n",
      "  BLEU: 0\n",
      "\n",
      "Overall Performance:\n",
      "  ROUGE-1: 0.0662\n",
      "  ROUGE-2: 0.0225\n",
      "  ROUGE-L: 0.0656\n",
      "  BLEU: 0.0\n",
      "\n",
      "==================== MODEL 3: Q → A_en ====================\n",
      "Training Loss: 5.127\n",
      "Answerable Examples: 200\n",
      "Unanswerable Examples: 0\n",
      "\n",
      "Answerable Performance:\n",
      "  ROUGE-1: 0.0976\n",
      "  ROUGE-2: 0.0301\n",
      "  ROUGE-L: 0.0978\n",
      "  BLEU: 0.0236\n",
      "\n",
      "Unanswerable Performance:\n",
      "  ROUGE-1: 0\n",
      "  ROUGE-2: 0\n",
      "  ROUGE-L: 0\n",
      "  BLEU: 0\n",
      "\n",
      "Overall Performance:\n",
      "  ROUGE-1: 0.0976\n",
      "  ROUGE-2: 0.0301\n",
      "  ROUGE-L: 0.0978\n",
      "  BLEU: 0.0236\n",
      "\n",
      "====================================================================================================\n",
      "SUMMARY TABLE\n",
      "====================================================================================================\n",
      "     Model  Train Loss  Answerable Count  Unanswerable Count  Answerable ROUGE-1  Unanswerable ROUGE-1  Overall ROUGE-1  Answerable BLEU  Unanswerable BLEU  Overall BLEU\n",
      "Q+C → A_te      3.2922               200                   0              0.5092                     0           0.5092           0.2541                  0        0.2541\n",
      "  Q → A_te      5.2182               200                   0              0.0662                     0           0.0662           0.0000                  0        0.0000\n",
      "  Q → A_en      5.1270               200                   0              0.0976                     0           0.0976           0.0236                  0        0.0236\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"lfs/hyperturing1/0/aarushs/hf_cache\"\n",
    "os.environ[\"HF_MODELS_CACHE\"] = \"lfs/hyperturing1/0/aarushs/hf_cache\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"lfs/hyperturing1/0/aarushs/hf_cache\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "dataset = load_dataset(\"coastalcph/tydi_xor_rc\")\n",
    "\n",
    "def filter_te_with_answers(example):\n",
    "    has_te_lang = example[\"lang\"] == \"te\"\n",
    "    has_answer = example[\"answer\"] is not None and len(example[\"answer\"]) > 0\n",
    "    return has_te_lang and has_answer\n",
    "\n",
    "train_dataset = dataset[\"train\"].filter(filter_te_with_answers)\n",
    "val_dataset = dataset[\"validation\"].filter(filter_te_with_answers)\n",
    "\n",
    "print(f\"Telugu training examples: {len(train_dataset)}\")\n",
    "print(f\"Telugu validation examples: {len(val_dataset)}\")\n",
    "\n",
    "model_checkpoint = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "max_length = 576\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED PROMPT FORMATTING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def format_prompt_qc(question, context):\n",
    "    \"\"\"\n",
    "    Improved prompt with clear structure and task instruction.\n",
    "    Uses delimiters and explicit task framing.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Answer the following question based on the given context. \"\n",
    "        f\"Provide a concise and accurate answer in Telugu.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"Context: {context}\\n\\n\"\n",
    "        f\"Answer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def format_prompt_q(question, lang=\"Telugu\"):\n",
    "    \"\"\"\n",
    "    Improved question-only prompt with task instruction.\n",
    "    Explicitly states the expected output language.\n",
    "    \"\"\"\n",
    "    if lang == \"Telugu\":\n",
    "        prompt = (\n",
    "            f\"Answer the following question accurately and concisely in Telugu. \"\n",
    "            f\"Use your knowledge to provide the best answer.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "    else:  # English\n",
    "        prompt = (\n",
    "            f\"Answer the following question accurately and concisely in English. \"\n",
    "            f\"Use your knowledge to provide the best answer.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Answer:\"\n",
    "        )\n",
    "    return prompt\n",
    "\n",
    "def format_prompt_qc_structured(question, context):\n",
    "    \"\"\"\n",
    "    Alternative: More structured format with XML-like tags.\n",
    "    Can help models better parse input components.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"<task>Answer the question in Telugu based on the context.</task>\\n\"\n",
    "        f\"<question>{question}</question>\\n\"\n",
    "        f\"<context>{context}</context>\\n\"\n",
    "        f\"<answer>\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def format_prompt_q_structured(question, lang=\"Telugu\"):\n",
    "    \"\"\"\n",
    "    Alternative: Structured format for question-only prompts.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"<task>Answer the question in {lang}.</task>\\n\"\n",
    "        f\"<question>{question}</question>\\n\"\n",
    "        f\"<answer>\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "def format_prompt_qc_instructional(question, context):\n",
    "    \"\"\"\n",
    "    Alternative: Instruction-following format.\n",
    "    Explicitly guides the model's behavior.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"### Instruction:\\n\"\n",
    "        f\"Read the context below and answer the question in Telugu. \"\n",
    "        f\"If the answer is not in the context, provide your best answer based on knowledge.\\n\\n\"\n",
    "        f\"### Context:\\n{context}\\n\\n\"\n",
    "        f\"### Question:\\n{question}\\n\\n\"\n",
    "        f\"### Response:\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# ============================================================================\n",
    "# PREPROCESSING FUNCTIONS (with improved prompts)\n",
    "# ============================================================================\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for q, c, a_inlang, a_en in zip(examples[\"question\"], examples[\"context\"], examples[\"answer_inlang\"], examples[\"answer\"]):\n",
    "        input_text = format_prompt_qc(q, c)\n",
    "        target_text = a_inlang if (a_inlang is not None and len(a_inlang) > 0) else (a_en if a_en is not None else \"\")\n",
    "        inputs.append(input_text)\n",
    "        targets.append(target_text)\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=False)\n",
    "    labels = tokenizer(targets, max_length=max_length, truncation=True, padding=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function_q_only(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for q, a_inlang, a_en in zip(examples[\"question\"], examples[\"answer_inlang\"], examples[\"answer\"]):\n",
    "        input_text = format_prompt_q(q, \"Telugu\")\n",
    "        target_text = a_inlang if (a_inlang is not None and len(a_inlang) > 0) else (a_en if a_en is not None else \"\")\n",
    "        inputs.append(input_text)\n",
    "        targets.append(target_text)\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=False)\n",
    "    labels = tokenizer(targets, max_length=max_length, truncation=True, padding=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_function_q_only_en_ans(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for q, a in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        input_text = format_prompt_q(q, \"English\")\n",
    "        target_text = a if a is not None else \"\"\n",
    "        inputs.append(input_text)\n",
    "        targets.append(target_text)\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=False)\n",
    "    labels = tokenizer(targets, max_length=max_length, truncation=True, padding=False)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"\\nTokenizing datasets...\")\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n",
    "tokenized_train_dataset_q_only = train_dataset.map(preprocess_function_q_only, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val_dataset_q_only = val_dataset.map(preprocess_function_q_only, batched=True, remove_columns=val_dataset.column_names)\n",
    "tokenized_train_dataset_q_only_en_ans = train_dataset.map(preprocess_function_q_only_en_ans, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_val_dataset_q_only_en_ans = val_dataset.map(preprocess_function_q_only_en_ans, batched=True, remove_columns=val_dataset.column_names)\n",
    "\n",
    "print(f\"Sample tokenized lengths:\")\n",
    "print(f\"  Input IDs: {len(tokenized_train_dataset[0]['input_ids'])}\")\n",
    "print(f\"  Labels: {len(tokenized_train_dataset[0]['labels'])}\")\n",
    "print(f\"  Non -100 labels: {sum(1 for x in tokenized_train_dataset[0]['labels'] if x != -100)}\")\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def train_and_eval(tokenized_train, tokenized_val, val_raw_dataset, output_dir, prompt_format_fn, answer_key=\"answer_inlang\"):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        model_checkpoint, \n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    if hasattr(model.config, \"dropout\"):\n",
    "        model.config.dropout = 0.28628261092381746\n",
    "    if hasattr(model.config, \"attention_dropout\"):\n",
    "        model.config.attention_dropout = 0.17909783477703217\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=0.0001400032301305189,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=6,\n",
    "        weight_decay=0.2880382450100906,\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=4,\n",
    "        bf16=True,\n",
    "        report_to=[],\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        dataloader_pin_memory=False,\n",
    "        max_grad_norm=1.8717849416806842,\n",
    "        warmup_ratio=0.2632858110781834,\n",
    "        warmup_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        lr_scheduler_type=\"cosine_with_restarts\",\n",
    "        logging_first_step=True,\n",
    "        label_smoothing_factor=0.11694641172038693,\n",
    "        adam_beta1=0.8806332791282017,\n",
    "        adam_beta2=0.9662516563582316,\n",
    "        adam_epsilon=1.972007726100591e-08,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_val,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "    model.eval()\n",
    "\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=100,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False,\n",
    "        num_beams=8,\n",
    "        repetition_penalty=1.6283771357674486,\n",
    "        length_penalty=1.5955228963302186,\n",
    "    )\n",
    "\n",
    "    answerable_predictions = []\n",
    "    answerable_references = []\n",
    "    unanswerable_predictions = []\n",
    "    unanswerable_references = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(200, len(val_raw_dataset))):\n",
    "            example = val_raw_dataset[i]\n",
    "            prompt = prompt_format_fn(example)\n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).input_ids.to(model.device)\n",
    "            outputs = model.generate(input_ids, **gen_kwargs)\n",
    "            pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            if example[\"answerable\"]:\n",
    "                if example[\"answer_inlang\"] and len(example[\"answer_inlang\"]) > 0:\n",
    "                    ref_text = example[\"answer_inlang\"]\n",
    "                else:\n",
    "                    ref_text = example[\"answer\"]\n",
    "                answerable_predictions.append(pred_text.strip())\n",
    "                answerable_references.append(ref_text.strip())\n",
    "            else:\n",
    "                unanswerable_predictions.append(pred_text.strip())\n",
    "                unanswerable_references.append(\"\")\n",
    "\n",
    "    answerable_rouge = rouge.compute(predictions=answerable_predictions, references=answerable_references, use_stemmer=True) if answerable_predictions else {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    try:\n",
    "        answerable_bleu = bleu.compute(predictions=answerable_predictions, references=[[r] for r in answerable_references]) if answerable_predictions else {\"bleu\": 0}\n",
    "    except ZeroDivisionError:\n",
    "        answerable_bleu = {\"bleu\": 0}\n",
    "    unanswerable_rouge = rouge.compute(predictions=unanswerable_predictions, references=unanswerable_references, use_stemmer=True) if unanswerable_predictions else {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    try:\n",
    "        unanswerable_bleu = bleu.compute(predictions=unanswerable_predictions, references=[[r] for r in unanswerable_references]) if unanswerable_predictions else {\"bleu\": 0}\n",
    "    except ZeroDivisionError:\n",
    "        unanswerable_bleu = {\"bleu\": 0}\n",
    "    all_predictions = answerable_predictions + unanswerable_predictions\n",
    "    all_references = answerable_references + unanswerable_references\n",
    "    overall_rouge = rouge.compute(predictions=all_predictions, references=all_references, use_stemmer=True) if all_predictions else {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "    try:\n",
    "        overall_bleu = bleu.compute(predictions=all_predictions, references=[[r] for r in all_references]) if all_predictions else {\"bleu\": 0}\n",
    "    except ZeroDivisionError:\n",
    "        overall_bleu = {\"bleu\": 0}\n",
    "    results = {\n",
    "        \"answerable_rouge1\": round(answerable_rouge.get(\"rouge1\", 0), 4),\n",
    "        \"answerable_rouge2\": round(answerable_rouge.get(\"rouge2\", 0), 4),\n",
    "        \"answerable_rougeL\": round(answerable_rouge.get(\"rougeL\", 0), 4),\n",
    "        \"answerable_bleu\": round(answerable_bleu.get(\"bleu\", 0), 4),\n",
    "        \"unanswerable_rouge1\": round(unanswerable_rouge.get(\"rouge1\", 0), 4),\n",
    "        \"unanswerable_rouge2\": round(unanswerable_rouge.get(\"rouge2\", 0), 4),\n",
    "        \"unanswerable_rougeL\": round(unanswerable_rouge.get(\"rougeL\", 0), 4),\n",
    "        \"unanswerable_bleu\": round(unanswerable_bleu.get(\"bleu\", 0), 4),\n",
    "        \"overall_rouge1\": round(overall_rouge.get(\"rouge1\", 0), 4),\n",
    "        \"overall_rouge2\": round(overall_rouge.get(\"rouge2\", 0), 4),\n",
    "        \"overall_rougeL\": round(overall_rouge.get(\"rougeL\", 0), 4),\n",
    "        \"overall_bleu\": round(overall_bleu.get(\"bleu\", 0), 4),\n",
    "        \"train_loss\": round(train_result.training_loss, 4),\n",
    "        \"answerable_count\": len(answerable_predictions),\n",
    "        \"unanswerable_count\": len(unanswerable_predictions)\n",
    "    }\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return results\n",
    "\n",
    "def prompt_fn_qc(example):\n",
    "    return format_prompt_qc(example[\"question\"], example[\"context\"])\n",
    "\n",
    "def prompt_fn_q_te(example):\n",
    "    return format_prompt_q(example[\"question\"], \"Telugu\")\n",
    "\n",
    "def prompt_fn_q_en(example):\n",
    "    return format_prompt_q(example[\"question\"], \"English\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Model 1: Telugu Question + English Context → Telugu Answer\")\n",
    "print(\"=\"*80)\n",
    "res1 = train_and_eval(tokenized_train_dataset, tokenized_val_dataset, val_dataset, \"mbart-te-qc\", prompt_fn_qc, \"answer_inlang\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Model 2: Telugu Question → Telugu Answer\")\n",
    "print(\"=\"*80)\n",
    "res2 = train_and_eval(tokenized_train_dataset_q_only, tokenized_val_dataset_q_only, val_dataset, \"mbart-te-q\", prompt_fn_q_te, \"answer_inlang\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Model 3: Telugu Question → English Answer\")\n",
    "print(\"=\"*80)\n",
    "res3 = train_and_eval(tokenized_train_dataset_q_only_en_ans, tokenized_val_dataset_q_only_en_ans, val_dataset, \"mbart-te-q-en\", prompt_fn_q_en, \"answer\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DETAILED RESULTS COMPARISON\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "results_data = []\n",
    "for i, (name, results) in enumerate([(\"Q+C → A_te\", res1), (\"Q → A_te\", res2), (\"Q → A_en\", res3)], 1):\n",
    "    print(f\"\\n{'='*20} MODEL {i}: {name} {'='*20}\")\n",
    "    print(f\"Training Loss: {results['train_loss']}\")\n",
    "    print(f\"Answerable Examples: {results['answerable_count']}\")\n",
    "    print(f\"Unanswerable Examples: {results['unanswerable_count']}\")\n",
    "    print(\"\\nAnswerable Performance:\")\n",
    "    print(f\"  ROUGE-1: {results['answerable_rouge1']}\")\n",
    "    print(f\"  ROUGE-2: {results['answerable_rouge2']}\")\n",
    "    print(f\"  ROUGE-L: {results['answerable_rougeL']}\")\n",
    "    print(f\"  BLEU: {results['answerable_bleu']}\")\n",
    "    print(\"\\nUnanswerable Performance:\")\n",
    "    print(f\"  ROUGE-1: {results['unanswerable_rouge1']}\")\n",
    "    print(f\"  ROUGE-2: {results['unanswerable_rouge2']}\")\n",
    "    print(f\"  ROUGE-L: {results['unanswerable_rougeL']}\")\n",
    "    print(f\"  BLEU: {results['unanswerable_bleu']}\")\n",
    "    print(\"\\nOverall Performance:\")\n",
    "    print(f\"  ROUGE-1: {results['overall_rouge1']}\")\n",
    "    print(f\"  ROUGE-2: {results['overall_rouge2']}\")\n",
    "    print(f\"  ROUGE-L: {results['overall_rougeL']}\")\n",
    "    print(f\"  BLEU: {results['overall_bleu']}\")\n",
    "    results_data.append({\n",
    "        'Model': name,\n",
    "        'Train Loss': results['train_loss'],\n",
    "        'Answerable Count': results['answerable_count'],\n",
    "        'Unanswerable Count': results['unanswerable_count'],\n",
    "        'Answerable ROUGE-1': results['answerable_rouge1'],\n",
    "        'Unanswerable ROUGE-1': results['unanswerable_rouge1'],\n",
    "        'Overall ROUGE-1': results['overall_rouge1'],\n",
    "        'Answerable BLEU': results['answerable_bleu'],\n",
    "        'Unanswerable BLEU': results['unanswerable_bleu'],\n",
    "        'Overall BLEU': results['overall_bleu']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(results_data)\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"SUMMARY TABLE\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
