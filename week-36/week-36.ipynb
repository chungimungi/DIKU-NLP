{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a9e0197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: lang\n",
      "bn    2598\n",
      "ar    2558\n",
      "ko    2422\n",
      "ja    2301\n",
      "fi    2126\n",
      "ru    1983\n",
      "te    1355\n",
      "Name: count, dtype: int64\n",
      "Validation size: lang\n",
      "fi    528\n",
      "bn    476\n",
      "ja    456\n",
      "ar    415\n",
      "ru    396\n",
      "te    384\n",
      "ko    356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
    "df_val = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])\n",
    "\n",
    "print(f\"Train size:\",df_train[\"lang\"].value_counts())\n",
    "print(f\"Validation size:\",df_val[\"lang\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Columns                                             question  \\\n",
      "0  à¦‰à¦‡à¦•à¦¿à¦²à¦¿à¦•à¦¸ à¦•à¦¤ à¦¸à¦¾à¦²à§‡ à¦¸à¦°à§à¦¬à¦ªà§à¦°à¦¥à¦® à¦‡à¦¨à§à¦Ÿà¦¾à¦°à¦¨à§‡à¦Ÿà§‡ à¦ªà§à¦°à¦¥à¦® à¦¤à¦¥...   \n",
      "\n",
      "                                             context lang  answerable  \\\n",
      "0  WikiLeaks () is an international non-profit or...   bn        True   \n",
      "\n",
      "   answer_start answer answer_inlang  \n",
      "0           182   2006          None  \n"
     ]
    }
   ],
   "source": [
    "print(\"First Columns\", df_train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43238ec",
   "metadata": {},
   "source": [
    "#### TO DO\n",
    "\n",
    "#### GENERAL\n",
    "- [âœ…] Shape\n",
    "- [âœ…] Word Count\n",
    "- [âœ…] Token Count\n",
    "\n",
    "#### SPECIFIC (For each language)ğŸ™ˆğŸ™ˆğŸ™ˆğŸ™ˆ\n",
    "- [âœ…] 5 Most common words + English translation\n",
    "- [âœ…] Analyze type of words\n",
    "- [] Rule based classifier (answerable or not)\n",
    "- [] Performance Evaluation (answerable field) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"ar\", \"ko\", \"te\", \"en\"]\n",
    "df_train = df_train[df_train[\"lang\"].isin(l)]\n",
    "df_val = df_val[df_val[\"lang\"].isin(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6335, 7)\n",
      "Validation shape: (1155, 7)\n",
      "lang\n",
      "ar    2558\n",
      "ko    2422\n",
      "te    1355\n",
      "Name: count, dtype: int64\n",
      "lang\n",
      "ar    415\n",
      "te    384\n",
      "ko    356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Validation shape: {df_val.shape}\")\n",
    "\n",
    "train_lan = df_train['lang'].value_counts()\n",
    "val_len = df_val['lang'].value_counts()\n",
    "print(train_lan)\n",
    "print(val_len) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts for Arabic: 16202\n",
      "Word counts for Korean: 11863\n",
      "Word counts for Telugu: 7690\n"
     ]
    }
   ],
   "source": [
    "ar, ko, te = df_train[df_train[\"lang\"] == \"ar\"], df_train[df_train[\"lang\"] == \"ko\"], df_train[df_train[\"lang\"] == \"te\"]\n",
    "\n",
    "def word_list(df):\n",
    "    words = [re.findall(r'\\w+', quest) for quest in df[\"question\"]]\n",
    "    \n",
    "    return [w for q in words for w in q]\n",
    "\n",
    "ar_words = word_list(ar)\n",
    "ko_words = word_list(ko)\n",
    "te_words = word_list(te)\n",
    "print(f\"Word counts for Arabic: {len(ar_words)}\")\n",
    "print(f\"Word counts for Korean: {len(ko_words)}\")\n",
    "print(f\"Word counts for Telugu: {len(te_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['30ë…„ ì „ìŸì˜ ìŠ¹ìëŠ” ëˆ„êµ¬ì¸ê°€?', 'ì—‘ìŠ¤ì„ ì€ ëˆ„ê°€ ë°œê²¬í•˜ì˜€ëŠ”ê°€?', 'ì•„í…Œë„¤ì—ì„œ ì–¸ì œ ê°€ì¥ ìµœê·¼ì˜ ì˜¬ë¦¼í”½ì´ ì˜¬ë ¸ë‚˜ìš”?', 'ì„¸ìƒì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ë°©ì†¡ì‚¬ëŠ” ë¬´ì—‡ì¸ê°€?', 'íŒ”ë ˆìŠ¤íƒ€ì¸ ìˆ˜ë„ëŠ” ì–´ë”˜ê°€ìš”?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [01:27<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the largest star on Earth?                 3\n",
      "How many countries have served in World War II?    3\n",
      "What is the largest bone in the human body?        3\n",
      "How many communist countries are there in 2019?    3\n",
      "When did the lead singer of N.EX.T. die?           2\n",
      "                                                  ..\n",
      "When was the first year that PSP was released?     1\n",
      "What is the biggest dinosaur?                      1\n",
      "How many World Cups has France hosted?             1\n",
      "What's the biggest city in China?                  1\n",
      "Who was the founder of the Nazi Party?             1\n",
      "Name: count, Length: 2367, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\aarus\\AppData\\Local\\Temp\\ipykernel_12672\\35628525.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ko[\"translation\"] = translations\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "print(ko[\"question\"].tolist()[:5])  \n",
    "\n",
    "tokenizer.src_lang = \"kor_Hang\"\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "\n",
    "translations = []\n",
    "batch_size = 16 \n",
    "\n",
    "for i in tqdm(range(0, len(ko), batch_size), desc=\"Translating\"):\n",
    "    batch = ko[\"question\"].iloc[i:i+batch_size].tolist()\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    trans = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    translations.extend(trans)\n",
    "\n",
    "ko[\"translation\"] = translations  \n",
    "\n",
    "print(pd.Series(translations).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language  Token Count\n",
      "0       ar        33733\n",
      "1       ko        25829\n",
      "2       te        18365\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "def token_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "results = []\n",
    "for lang_name, df in [(\"ar\", ar), (\"ko\", ko), (\"te\", te)]:\n",
    "    results.append({\n",
    "        \"Language\": lang_name,\n",
    "        \"Token Count\": df[\"question\"].apply(token_count).sum(),\n",
    "    })\n",
    "\n",
    "token_counts = pd.DataFrame(results)\n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Most common words ğŸ™ˆ\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "\n",
    "ar_text = \" \".join(ar_words)\n",
    "reshaped_text = arabic_reshaper.reshape(ar_text)\n",
    "bidi_text = get_display(reshaped_text)\n",
    "ar_wordcloud = WordCloud(font_path='arial.ttf', background_color='white').generate(bidi_text)\n",
    "\n",
    "ko_text = \" \".join(ko_words)\n",
    "ko_wordcloud = WordCloud(font_path='malgun.ttf', background_color='white').generate(ko_text)\n",
    "\n",
    "te_text = \" \".join(te_words)\n",
    "te_wordcloud = WordCloud(font_path='gautami.ttf', background_color='white').generate(te_text)\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=600)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(ar_wordcloud, interpolation='bilinear')\n",
    "plt.title('Arabic')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(ko_wordcloud, interpolation='bilinear')\n",
    "plt.title('Korean')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(te_wordcloud, interpolation='bilinear')\n",
    "plt.title('Telugu')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fc04b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ë“œë˜ê³¤ ì‹ í™”ëŠ” ì–´ëŠ ë‚˜ë¼ì—ì„œ ì‹œì‘ ë˜ì—ˆëŠ”ê°€?', 'ì €ì˜í–¥ ê°œë°œì˜ ì¥ì ì€ ë¬´ì—‡ì¸ê°€?', 'ì—­ì‚¬ìƒ ê°€ì¥ ë§ì€ ì‚¬ìƒìë¥¼ ë¶ˆëŸ¬ì˜¨ ì „íˆ¬ëŠ” ë¬´ì—‡ì¸ê°€?', 'ë¯¸êµ­ ë‚¨ë¶ì „ìŸì€ ì–¸ì œ ëë‚¬ëŠ”ê°€?', 'ì˜¤ìŠ¤ë§Œ ì œêµ­ì˜ 1ëŒ€ ì™•ì€ ëˆ„êµ¬ì¸ê°€ìš”?']\n",
      "['In which country did the myth of the dragon originate?', \"What's the advantage of low-level development?\", 'What is the most deadly battle in history?', 'When did the American Civil War end?', 'Who was the first king of the Ottoman Empire?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "ko = ko.sample(5)\n",
    "print(ko[\"question\"].tolist())\n",
    "\n",
    "tokenizer.src_lang = \"kor_Hang\"\n",
    "inputs = tokenizer(ko[\"question\"].tolist(), return_tensors=\"pt\", padding=True)\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "\n",
    "outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09d27e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RULES\n",
    "# \n",
    "# Who             (Proper Nouns)\n",
    "# What            (???)\n",
    "# When            (Dates)\n",
    "# Where           (Proper Nouns)\n",
    "# Why             (Because)\n",
    "# How             (Bet on Yes)\n",
    "# Whose           (Proper Nouns)\n",
    "# Which           (Bet on Yes)\n",
    "# How many/much   (Quantity)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65b900f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating ar: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [01:12<00:00,  1.39s/it]\n",
      "Translating ar: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52/52 [45:24<00:00, 52.39s/it]\n",
      "Predicting ar: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [00:00<00:00, 29342.17it/s]\n",
      "Translating ko: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [00:57<00:00,  1.28s/it]\n",
      "Translating ko: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45/45 [37:46<00:00, 50.37s/it]\n",
      "Predicting ko: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 356/356 [00:00<00:00, 29267.56it/s]\n",
      "Translating te: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [01:04<00:00,  1.35s/it]\n",
      "Translating te: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [33:19<00:00, 41.65s/it]\n",
      "Predicting te: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 384/384 [00:00<00:00, 32483.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance by language:\n",
      "ar {'accuracy': 0.7975903614457831, 'precision': 0.8780487804878049, 'recall': 0.8925619834710744, 'f1': 0.8852459016393442}\n",
      "ko {'accuracy': 0.8258426966292135, 'precision': 0.9508196721311475, 'recall': 0.8605341246290801, 'f1': 0.9034267912772586}\n",
      "te {'accuracy': 0.7864583333333334, 'precision': 0.8028985507246377, 'recall': 0.9518900343642611, 'f1': 0.8710691823899371}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
    "df_val = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "lang_codes = {\"ar\": \"ara_Arab\", \"ko\": \"kor_Hang\", \"te\": \"tel_Telu\"}\n",
    "\n",
    "def translate_to_en(texts, src_lang):\n",
    "    tokenizer.src_lang = lang_codes[src_lang]\n",
    "    outputs = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), 8), desc=f\"Translating {src_lang}\"):\n",
    "        batch = texts[i:i+8]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        bos_token_id = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "        \n",
    "        out = model.generate(**inputs, forced_bos_token_id=bos_token_id)\n",
    "        outputs.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    stopwords = {\"the\",\"is\",\"a\",\"an\",\"in\",\"on\",\"at\",\"and\",\"of\",\"to\",\"for\",\"with\"}\n",
    "    return set(t for t in tokens if t not in stopwords)\n",
    "\n",
    "def predict_answerable(question, context):\n",
    "    return 1 if len(preprocess(question) & preprocess(context)) > 0 else 0\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lang in [\"ar\", \"ko\", \"te\"]:\n",
    "    subset = df_val[df_val[\"lang\"] == lang].copy()\n",
    "    \n",
    "    subset[\"question_en\"] = translate_to_en(subset[\"question\"].tolist(), lang)\n",
    "    subset[\"context_en\"] = translate_to_en(subset[\"context\"].tolist(), lang)\n",
    "    \n",
    "    subset[\"pred\"] = [predict_answerable(q, c) for q, c in tqdm(zip(subset[\"question_en\"], subset[\"context_en\"]), total=len(subset), desc=f\"Predicting {lang}\")]\n",
    "    \n",
    "    acc = accuracy_score(subset[\"answerable\"], subset[\"pred\"])\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(subset[\"answerable\"], subset[\"pred\"], average=\"binary\")\n",
    "    \n",
    "    results[lang] = {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "print(\"Performance by language:\")\n",
    "for lang, metrics in results.items():\n",
    "    print(lang, metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
