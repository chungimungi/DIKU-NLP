{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a9e0197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: lang\n",
      "bn    2598\n",
      "ar    2558\n",
      "ko    2422\n",
      "ja    2301\n",
      "fi    2126\n",
      "ru    1983\n",
      "te    1355\n",
      "Name: count, dtype: int64\n",
      "Validation size: lang\n",
      "fi    528\n",
      "bn    476\n",
      "ja    456\n",
      "ar    415\n",
      "ru    396\n",
      "te    384\n",
      "ko    356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
    "df_val = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])\n",
    "\n",
    "print(f\"Train size:\",df_train[\"lang\"].value_counts())\n",
    "print(f\"Validation size:\",df_val[\"lang\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Columns                                             question  \\\n",
      "0  উইকিলিকস কত সালে সর্বপ্রথম ইন্টারনেটে প্রথম তথ...   \n",
      "\n",
      "                                             context lang  answerable  \\\n",
      "0  WikiLeaks () is an international non-profit or...   bn        True   \n",
      "\n",
      "   answer_start answer answer_inlang  \n",
      "0           182   2006          None  \n"
     ]
    }
   ],
   "source": [
    "print(\"First Columns\", df_train.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43238ec",
   "metadata": {},
   "source": [
    "#### TO DO\n",
    "\n",
    "#### GENERAL\n",
    "- [✅] Shape\n",
    "- [✅] Word Count\n",
    "- [✅] Token Count\n",
    "\n",
    "#### SPECIFIC (For each language)🙈🙈🙈🙈\n",
    "- [✅] 5 Most common words + English translation\n",
    "- [✅] Analyze type of words\n",
    "- [] Rule based classifier (answerable or not)\n",
    "- [] Performance Evaluation (answerable field) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"ar\", \"ko\", \"te\", \"en\"]\n",
    "df_train = df_train[df_train[\"lang\"].isin(l)]\n",
    "df_val = df_val[df_val[\"lang\"].isin(l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6335, 7)\n",
      "Validation shape: (1155, 7)\n",
      "lang\n",
      "ar    2558\n",
      "ko    2422\n",
      "te    1355\n",
      "Name: count, dtype: int64\n",
      "lang\n",
      "ar    415\n",
      "te    384\n",
      "ko    356\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {df_train.shape}\")\n",
    "print(f\"Validation shape: {df_val.shape}\")\n",
    "\n",
    "train_lan = df_train['lang'].value_counts()\n",
    "val_len = df_val['lang'].value_counts()\n",
    "print(train_lan)\n",
    "print(val_len) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts for Arabic: 16202\n",
      "Word counts for Korean: 11863\n",
      "Word counts for Telugu: 7690\n"
     ]
    }
   ],
   "source": [
    "ar, ko, te = df_train[df_train[\"lang\"] == \"ar\"], df_train[df_train[\"lang\"] == \"ko\"], df_train[df_train[\"lang\"] == \"te\"]\n",
    "\n",
    "def word_list(df):\n",
    "    words = [re.findall(r'\\w+', quest) for quest in df[\"question\"]]\n",
    "    \n",
    "    return [w for q in words for w in q]\n",
    "\n",
    "ar_words = word_list(ar)\n",
    "ko_words = word_list(ko)\n",
    "te_words = word_list(te)\n",
    "print(f\"Word counts for Arabic: {len(ar_words)}\")\n",
    "print(f\"Word counts for Korean: {len(ko_words)}\")\n",
    "print(f\"Word counts for Telugu: {len(te_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['30년 전쟁의 승자는 누구인가?', '엑스선은 누가 발견하였는가?', '아테네에서 언제 가장 최근의 올림픽이 올렸나요?', '세상에서 가장 오래된 방송사는 무엇인가?', '팔레스타인 수도는 어딘가요?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 152/152 [01:27<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the largest star on Earth?                 3\n",
      "How many countries have served in World War II?    3\n",
      "What is the largest bone in the human body?        3\n",
      "How many communist countries are there in 2019?    3\n",
      "When did the lead singer of N.EX.T. die?           2\n",
      "                                                  ..\n",
      "When was the first year that PSP was released?     1\n",
      "What is the biggest dinosaur?                      1\n",
      "How many World Cups has France hosted?             1\n",
      "What's the biggest city in China?                  1\n",
      "Who was the founder of the Nazi Party?             1\n",
      "Name: count, Length: 2367, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\aarus\\AppData\\Local\\Temp\\ipykernel_12672\\35628525.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ko[\"translation\"] = translations\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "print(ko[\"question\"].tolist()[:5])  \n",
    "\n",
    "tokenizer.src_lang = \"kor_Hang\"\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "\n",
    "translations = []\n",
    "batch_size = 16 \n",
    "\n",
    "for i in tqdm(range(0, len(ko), batch_size), desc=\"Translating\"):\n",
    "    batch = ko[\"question\"].iloc[i:i+batch_size].tolist()\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "    trans = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    translations.extend(trans)\n",
    "\n",
    "ko[\"translation\"] = translations  \n",
    "\n",
    "print(pd.Series(translations).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Language  Token Count\n",
      "0       ar        33733\n",
      "1       ko        25829\n",
      "2       te        18365\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "def token_count(text):\n",
    "    return len(tokenizer.tokenize(text))\n",
    "\n",
    "results = []\n",
    "for lang_name, df in [(\"ar\", ar), (\"ko\", ko), (\"te\", te)]:\n",
    "    results.append({\n",
    "        \"Language\": lang_name,\n",
    "        \"Token Count\": df[\"question\"].apply(token_count).sum(),\n",
    "    })\n",
    "\n",
    "token_counts = pd.DataFrame(results)\n",
    "print(token_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Most common words 🙈\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "\n",
    "ar_text = \" \".join(ar_words)\n",
    "reshaped_text = arabic_reshaper.reshape(ar_text)\n",
    "bidi_text = get_display(reshaped_text)\n",
    "ar_wordcloud = WordCloud(font_path='arial.ttf', background_color='white').generate(bidi_text)\n",
    "\n",
    "ko_text = \" \".join(ko_words)\n",
    "ko_wordcloud = WordCloud(font_path='malgun.ttf', background_color='white').generate(ko_text)\n",
    "\n",
    "te_text = \" \".join(te_words)\n",
    "te_wordcloud = WordCloud(font_path='gautami.ttf', background_color='white').generate(te_text)\n",
    "\n",
    "plt.figure(figsize=(15, 5), dpi=600)\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(ar_wordcloud, interpolation='bilinear')\n",
    "plt.title('Arabic')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(ko_wordcloud, interpolation='bilinear')\n",
    "plt.title('Korean')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(te_wordcloud, interpolation='bilinear')\n",
    "plt.title('Telugu')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fc04b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['드래곤 신화는 어느 나라에서 시작 되었는가?', '저영향 개발의 장점은 무엇인가?', '역사상 가장 많은 사상자를 불러온 전투는 무엇인가?', '미국 남북전쟁은 언제 끝났는가?', '오스만 제국의 1대 왕은 누구인가요?']\n",
      "['In which country did the myth of the dragon originate?', \"What's the advantage of low-level development?\", 'What is the most deadly battle in history?', 'When did the American Civil War end?', 'Who was the first king of the Ottoman Empire?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "ko = ko.sample(5)\n",
    "print(ko[\"question\"].tolist())\n",
    "\n",
    "tokenizer.src_lang = \"kor_Hang\"\n",
    "inputs = tokenizer(ko[\"question\"].tolist(), return_tensors=\"pt\", padding=True)\n",
    "forced_bos_token_id = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "\n",
    "outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09d27e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RULES\n",
    "# \n",
    "# Who             (Proper Nouns)\n",
    "# What            (???)\n",
    "# When            (Dates)\n",
    "# Where           (Proper Nouns)\n",
    "# Why             (Because)\n",
    "# How             (Bet on Yes)\n",
    "# Whose           (Proper Nouns)\n",
    "# Which           (Bet on Yes)\n",
    "# How many/much   (Quantity)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65b900f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating ar: 100%|██████████| 52/52 [01:12<00:00,  1.39s/it]\n",
      "Translating ar: 100%|██████████| 52/52 [45:24<00:00, 52.39s/it]\n",
      "Predicting ar: 100%|██████████| 415/415 [00:00<00:00, 29342.17it/s]\n",
      "Translating ko: 100%|██████████| 45/45 [00:57<00:00,  1.28s/it]\n",
      "Translating ko: 100%|██████████| 45/45 [37:46<00:00, 50.37s/it]\n",
      "Predicting ko: 100%|██████████| 356/356 [00:00<00:00, 29267.56it/s]\n",
      "Translating te: 100%|██████████| 48/48 [01:04<00:00,  1.35s/it]\n",
      "Translating te: 100%|██████████| 48/48 [33:19<00:00, 41.65s/it]\n",
      "Predicting te: 100%|██████████| 384/384 [00:00<00:00, 32483.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance by language:\n",
      "ar {'accuracy': 0.7975903614457831, 'precision': 0.8780487804878049, 'recall': 0.8925619834710744, 'f1': 0.8852459016393442}\n",
      "ko {'accuracy': 0.8258426966292135, 'precision': 0.9508196721311475, 'recall': 0.8605341246290801, 'f1': 0.9034267912772586}\n",
      "te {'accuracy': 0.7864583333333334, 'precision': 0.8028985507246377, 'recall': 0.9518900343642611, 'f1': 0.8710691823899371}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
    "df_val = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "lang_codes = {\"ar\": \"ara_Arab\", \"ko\": \"kor_Hang\", \"te\": \"tel_Telu\"}\n",
    "\n",
    "def translate_to_en(texts, src_lang):\n",
    "    tokenizer.src_lang = lang_codes[src_lang]\n",
    "    outputs = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), 8), desc=f\"Translating {src_lang}\"):\n",
    "        batch = texts[i:i+8]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        bos_token_id = tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    "        \n",
    "        out = model.generate(**inputs, forced_bos_token_id=bos_token_id)\n",
    "        outputs.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    stopwords = {\"the\",\"is\",\"a\",\"an\",\"in\",\"on\",\"at\",\"and\",\"of\",\"to\",\"for\",\"with\"}\n",
    "    return set(t for t in tokens if t not in stopwords)\n",
    "\n",
    "def predict_answerable(question, context):\n",
    "    return 1 if len(preprocess(question) & preprocess(context)) > 0 else 0\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lang in [\"ar\", \"ko\", \"te\"]:\n",
    "    subset = df_val[df_val[\"lang\"] == lang].copy()\n",
    "    \n",
    "    subset[\"question_en\"] = translate_to_en(subset[\"question\"].tolist(), lang)\n",
    "    subset[\"context_en\"] = translate_to_en(subset[\"context\"].tolist(), lang)\n",
    "    \n",
    "    subset[\"pred\"] = [predict_answerable(q, c) for q, c in tqdm(zip(subset[\"question_en\"], subset[\"context_en\"]), total=len(subset), desc=f\"Predicting {lang}\")]\n",
    "    \n",
    "    acc = accuracy_score(subset[\"answerable\"], subset[\"pred\"])\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(subset[\"answerable\"], subset[\"pred\"], average=\"binary\")\n",
    "    \n",
    "    results[lang] = {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "print(\"Performance by language:\")\n",
    "for lang, metrics in results.items():\n",
    "    print(lang, metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
